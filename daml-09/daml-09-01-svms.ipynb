{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAML 09 - SVMs in Action\n",
    "\n",
    "Michal Grochmal <michal.grochmal@city.ac.uk>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support vector machine was, for a long time, one of the most advanced ML techniques.\n",
    "It fits a surface between the classes (or through a regressed function)\n",
    "in a way to contain as big margins towards the data points as possible.\n",
    "For classification the SVM attempts to have as big margins\n",
    "(distances from the fitted surface) as possible form the classes;\n",
    "for regression the SVM attempt to fit as many data points within the margins.\n",
    "\n",
    "SVMs are therefore often called *max-margin* classifiers (or regressors).\n",
    "The technique works very well with high dimensionality but becomes considerably\n",
    "slow when faced with too many (millions) samples.\n",
    "In other words the SVMs were replaced by newer techniques in big data processing.\n",
    "That said, SVMs are some of the best binary classifiers when not faces with millions of samples.\n",
    "Let's try to find good ways of classifying two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify two classes we want something that separates them.\n",
    "The simple (perhaps naive) approach is to:\n",
    "\n",
    "1. find the center of each class\n",
    "2. draw a line between the two centers\n",
    "3. find the middle point between the centers\n",
    "4. find the perpendicular line that passes through the middle point\n",
    "5. use this last line as the decision function between the two classes\n",
    "\n",
    "This, let's call it *mean distance separator*, works.\n",
    "In practice it performs a very simple algorithm of finding the closest class center,\n",
    "we can say that it does.\n",
    "\n",
    "$$\n",
    "\\min_{c} p \\cdot c\n",
    "$$\n",
    "\n",
    "This is very fast and very easy to code.\n",
    "We could even say that it is a linear form of Naive Bayes.\n",
    "Yet, this is not very accurate when classes are close or even interleaved with each other.\n",
    "Let's do some algebra and try this algorithm out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "blob_range = (-15, 15)\n",
    "blobs = []\n",
    "n_blob = 3\n",
    "for i in range(n_blob):\n",
    "    blobs.append(make_blobs(64, centers=2, cluster_std=2.2))\n",
    "\n",
    "def naive_separation(blobs, blob_num, ax):\n",
    "    X, y = blobs[blob_num]\n",
    "    center0 = X[y == 0, :].mean(axis=0)\n",
    "    center1 = X[y == 1, :].mean(axis=0)\n",
    "    incline = (center1[1] - center0[1]) / (center1[0] - center0[0])\n",
    "    intercept = center0[1] - incline*center0[0]\n",
    "    middle = (center1 + center0)/2\n",
    "    invinc = -1/incline\n",
    "    invinter = middle[1] - invinc*middle[0]\n",
    "\n",
    "    x_fun = np.linspace(*blob_range, 10)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='prism')\n",
    "    ax.scatter(center0[0], center0[1], c='navy')\n",
    "    ax.scatter(center1[0], center1[1], c='navy')\n",
    "    ax.scatter(middle[0], middle[1], c='navy')\n",
    "    ax.plot(x_fun, incline*x_fun + intercept, color='navy', linestyle=':')\n",
    "    ax.plot(x_fun, invinc*x_fun + invinter, color='navy', linestyle='-')\n",
    "    ax.set(xlim=blob_range, ylim=blob_range)\n",
    "\n",
    "fig, axi = plt.subplots(1, n_blob, figsize=(16, 7))\n",
    "for i in range(n_blob):\n",
    "    naive_separation(blobs, i, axi.flat[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it gets it right, sometimes not quite.\n",
    "And in higher dimensional space this becomes worse,\n",
    "because the algorithm does not have a way to tune the slope in specific dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead let's try a **Hard margin** SVM.\n",
    "This attempt to build a margin between the decision (hyper)plane and the closest samples of each class.\n",
    "We want a big margin therefore we attempt to fit a (hyper)plane through the current dimensions.\n",
    "The slope of this (hyper)plane is $w$, which is a vector since we have a slope in each dimension.\n",
    "This algorithms attempts to minimize the following.\n",
    "\n",
    "$$\n",
    "\\min_{w, b} \\frac{1}{2} w^T \\cdot w \\\\\n",
    "\\texttt{subject to } t^i (w^T \\cdot x^i + b) \\ge 1, \\texttt{ for i as every data point}\n",
    "$$\n",
    "\n",
    "This is a form of *quadratic programming* (QP),\n",
    "which attempts to solve a convex problem with *constraints*.\n",
    "Several QP procedures exist in `scipy` but `SVC` (Support Vector Classifier) already does this for us.\n",
    "We will try to fit such a (hyper)plane - here a line- on the blobs above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def svc_separation(blobs, blob_num, ax, svm_C=200):\n",
    "    X, y = blobs[blob_num]\n",
    "    model = SVC(C=svm_C, kernel='linear')\n",
    "    model.fit(X, y)\n",
    "\n",
    "    incline = - model.coef_[0][0] / model.coef_[0][1]\n",
    "    x_decision = np.linspace(*blob_range, 128)\n",
    "    y_decision = incline*x_decision - model.intercept_[0] / model.coef_[0][1]\n",
    "\n",
    "    margin = 1 / np.sqrt(np.sum(model.coef_[0] ** 2))\n",
    "    y_up = y_decision + np.sqrt(1 + incline ** 2) * margin\n",
    "    y_down = y_decision - np.sqrt(1 + incline ** 2) * margin\n",
    "\n",
    "    ax.fill_between(x_decision, -15, y_decision, facecolor='steelblue', alpha=0.5)\n",
    "    ax.fill_between(x_decision, 15, y_decision, facecolor='wheat', alpha=0.5)\n",
    "    ax.plot(x_decision, y_decision, color='navy')\n",
    "    ax.plot(x_decision, y_up, color='navy', linestyle='-.')\n",
    "    ax.plot(x_decision, y_down, color='navy', linestyle='-.')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='prism')\n",
    "    ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "           facecolor='none', edgecolors='navy', linewidths=2)\n",
    "    ax.set(xlim=blob_range, ylim=blob_range)\n",
    "\n",
    "fig, axi = plt.subplots(1, n_blob, figsize=(16, 7))\n",
    "for i in range(n_blob):\n",
    "    svc_separation(blobs, i, axi.flat[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better with the margins we often get a decision function that would generalize better.\n",
    "Yet, if the margin is small, we often see that we could find a better function.\n",
    "Enter **Soft margin SVM**.\n",
    "If we could allow some points to be inside the margin,\n",
    "we would make for bigger margins.\n",
    "We add an extra parameter to the minimization problem.\n",
    "\n",
    "$$\n",
    "\\min_{w, b} \\frac{1}{2} w^T \\cdot w + C \\sum \\zeta^i\\\\\n",
    "\\texttt{subject to } t^i (w^T \\cdot x^i + b) \\ge 1 - \\zeta^i, \\texttt{ for i as every data point}\n",
    "$$\n",
    "\n",
    "This translates into a model hyperparameter the $C$ in SVMs.\n",
    "The smaller the parameter the more points we can allow within the margin.\n",
    "The $C$ hyperparameter is an *inverse regularization*,\n",
    "it works in a similar way to model regularization\n",
    "but the smaller its value the more regularized the model is.\n",
    "Let's try again with a small $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axi = plt.subplots(1, n_blob, figsize=(16, 7))\n",
    "for i in range(n_blob):\n",
    "    svc_separation(blobs, i, axi.flat[i], svm_C=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got better margins alright.\n",
    "We  have been marking the *support vectors* in the graphs above.\n",
    "The support vectors are the points within or on the border of the margins.\n",
    "A trained SVM keeps track of the support vectors only and classifies based on that alone.\n",
    "This also means that the smaller $C$ is the model is likely better\n",
    "but also uses more memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels\n",
    "\n",
    "But what is a line clearly won't do?\n",
    "The real power of SVMs is the fact that they work well in very high dimensions,\n",
    "therefore we can throw our data into a very high dimensional space and fit the hyperplane there.\n",
    "Here is a blob that cannot be easily classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(64, centers=2, cluster_std=3, random_state=42)\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap='prism');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could add polynomial features to it and then fit the SVM, and that will work.\n",
    "Yet the SVM itself can do it for us.\n",
    "It has a `kernel=` parameter which we will set to `poly` (polynomial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='poly', C=0.3)\n",
    "model.fit(X, y)\n",
    "xg, yg = np.mgrid[-10:10:64j, -10:10:64j]\n",
    "zg = model.decision_function(np.c_[xg.ravel(), yg.ravel()])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap='prism');\n",
    "ax.contour(xg, yg, zg.reshape(xg.shape),\n",
    "           colors='navy', linestyles=['-.', '-', '-.'], levels=[-.5, 0, .5])\n",
    "ax.set(xlim=(-10, 10), ylim=(-10, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you try a pipeline with polynomial features and then compare against a polynomial SVM,\n",
    "you will find that the SVM is much faster.  Why is that?\n",
    "\n",
    "During the solution to the SVM QP problem a *kernel trick* is possible.\n",
    "The kernel trick is a simplification that uses a kernel identity\n",
    "that simplifies the higher dimensional space to a much simple form.\n",
    "In other words, instead of actually projecting the data into higher dimension space,\n",
    "we keep that data in the same dimensions but substitute a kernel expression once\n",
    "the data in high dimensional space is needed.\n",
    "\n",
    "All this only works because of a specific way of solving QP problems,\n",
    "and the fact that a kernel must abide by several properties.\n",
    "The identity is that we can define a kernel $\\kappa$ so that the following holds true\n",
    "(under specific circumstances).\n",
    "\n",
    "$$\n",
    "\\kappa(x) \\cdot \\kappa(y) = \\kappa(x, y)\n",
    "$$\n",
    "\n",
    "This prevents us from calculating a, possibly huge, dot product.\n",
    "The typical SVM kernels are as follows.\n",
    "\n",
    "- Linear Kernel (`linear`)\n",
    "  $\\kappa(x) = x \\leftrightarrow \\kappa(x, y) = x^T \\cdot y$\n",
    "- Polynomial Kernel (`poly`)\n",
    "  $\\kappa(x) = x^d \\leftrightarrow \\kappa(x, y) = (\\eta x \\cdot y + \\rho)^d$\n",
    "- Radial Basis Function Kernel (`rbf`)\n",
    "  $\\kappa(x) = e^{-x^2} \\leftrightarrow \\kappa(x, y) = e^{-\\eta (x - y)^2}$\n",
    "\n",
    "We can try out how just the kernel itself works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_circles\n",
    "X, y = make_circles(64, factor=0.2, noise=0.1)\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap='prism');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither a line or a polynomial will do here.\n",
    "But we have that Radial Basis Function Kernel, and the equation for a simple\n",
    "*Gaussian Radial Basis Function* is:\n",
    "\n",
    "$$\n",
    "\\phi(x) = e^{-x^2}\n",
    "$$\n",
    "\n",
    "Let's apply that as a way of throwing the data into higher dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "grbf = np.exp(-(X ** 2).sum(axis=1))\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], grbf, c=y, s=100, cmap='prism');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it is quite easy to fit a (hyper)plane through the middle of the classes now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Olivetti Faces\n",
    "\n",
    "SVMs are good for image recognition because of the high dimensional space,\n",
    "i.e. one dimension per pixel in the image.\n",
    "We will first attempt to classify a dataset in which faces are well centered in the images.\n",
    "The Olivetti faces is a set of 400 images of faces, 10 faces per person.\n",
    "It is a very clean dataset: the faces are well centered and the *support* of each class\n",
    "(number of instances) is the same across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "ofaces = fetch_olivetti_faces()\n",
    "print(ofaces.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also look at the images to see what we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(16, 16), subplot_kw={'xticks':[], 'yticks':[]})\n",
    "fig.subplots_adjust(hspace=0, wspace=0)\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(ofaces.images[i*2], cmap='binary_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can certainly see what was the glasses tren in the early 1990s!\n",
    "\n",
    "This dataset is very ordered, each person's faces (class) are next to each other.\n",
    "If we will want to do cross validation we will need to shuffle it first,\n",
    "that is what `sklearn.model_selection.KFold` allows us to do.\n",
    "\n",
    "We did use `GridSearchCV` before but not with a pipeline.\n",
    "With a pipeline the grid search can be given hyperparameter lists\n",
    "with the name of the part of the pipeline, a double underscore and the hyperparameter name.\n",
    "This is accomplished by some clever processing inside the `set_params` method,\n",
    "that exists in each model (or pipeline) in `sklearn`.\n",
    "The double underscore means to invoke `set_params` on the attribute named\n",
    "before the double underscore, and it can be repeated when dealing with deep pipelines.\n",
    "\n",
    "Also, the dictionary parameter to the grid search can be a list of dictionaries - as here.\n",
    "This allows for different combinations when using different hyperparameters.\n",
    "For example, here we try different kernels, and each requires a different set of hyperparameters.\n",
    "The following will take a while to complete when run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "model = make_pipeline(PCA(n_components=200, svd_solver='randomized'), SVC())\n",
    "strategy = KFold(n_splits=5, shuffle=True)\n",
    "param_grid = [\n",
    "    {'svc__kernel': ['linear'], 'svc__C': [1, 10, 100, 1000, 10000]},\n",
    "    {'svc__kernel': ['rbf'], 'svc__C': [1, 10, 1000, 10000], 'svc__gamma': [0.001, 0.1, 1.0, 10.0]}\n",
    "]\n",
    "grid = GridSearchCV(model, param_grid, cv=strategy)\n",
    "grid.fit(ofaces.data, ofaces.target)\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a pretty good score.\n",
    "SVMs are a good method for classifying images.\n",
    "Let's have a look at what hyperparameters were used, throughout the entire pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $C$ parameter of an SVM is its tolerance to data points inside the support vectors.\n",
    "A smaller $C$ results in a model with better generalization\n",
    "(better at classifying unknown data) because it is less sensitive to noise in the data.\n",
    "Yet, a too small $C$ will undefit the data.\n",
    "\n",
    "That grid search took a while but it could only try a handful of values for $C$.\n",
    "There is a way of performing a slightly faster search\n",
    "by limiting the number of iterations whilst trying some numbers at random.\n",
    "Yet, a random search cannot take different dictionaries so we will need to make do\n",
    "with a single dictionary of all parameters.\n",
    "Also, here we use `scipy.stats` random distributions to gather numbers from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "model = make_pipeline(PCA(n_components=200, svd_solver='randomized'), SVC())\n",
    "strategy = KFold(n_splits=5, shuffle=True)\n",
    "param_dist = {\n",
    "    'svc__kernel': ['linear', 'rbf'],\n",
    "    'svc__C': sp_randint(1, 100000),\n",
    "    'svc__gamma': uniform(0.001, 10.0)\n",
    "}\n",
    "search = RandomizedSearchCV(model, param_dist, cv=strategy, n_iter=20)\n",
    "search.fit(ofaces.data, ofaces.target)\n",
    "search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still takes a while.\n",
    "Deciding between a grid search or a randomized search is a choice based on our knowledge of the data.\n",
    "It is likely that the randomized search found a better $C$ value for the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we cannot really know the actual generalization score for these hyperparameters.\n",
    "This is because we tuned the hyperparameters against all data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicalss SVMs\n",
    "\n",
    "Have you noticed something strange?\n",
    "I just told you that an SVM draws a (hyper)plane between two classes but we just\n",
    "classified 40 different classes.  What magic is happening here?\n",
    "\n",
    "In reality we have trained 780 different SVMs, one for each pair of classes $(combination(40, 2) = 780)$.\n",
    "An SVM is a **binary classifier**, i.e. it can only classify between two classes,\n",
    "often called the *negative class* and the *positive class*.\n",
    "What the Support Vector Classifier (`SVC`) performs is a technique called *One vs One* (OVO),\n",
    "in which it trains a classifier for each pair of classes\n",
    "and then runs all the classifiers to perform the prediction.\n",
    "\n",
    "One vs All (OVA, or OVR - One vs Rest) is another technique to make\n",
    "a binary classifier work on several classes.\n",
    "Whilst OVO trains a classifier for each pair of classes on subsets of data,\n",
    "OVA trains a classifier for each class considering it as the positive class\n",
    "and all other classes as the negative class.\n",
    "\n",
    "OVO trains more classifiers but OVA trains all classifiers on the full dataset.\n",
    "For SVMs OVO is faster because an SVM takes much longer to train when the amount of data increases,\n",
    "i.e. training more small SVMs is faster than a few big SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Wold Problem\n",
    "\n",
    "Classification issues on the Olivetti faces are hard to visualize because out classes are just numbers.\n",
    "Let's use the Labeled Faces in the Wild dataset again,\n",
    "since we can recognize some of the people in there by ourselves.\n",
    "\n",
    "LFW is a much harder dataset:\n",
    "the faces are in different orientations and the support of distinct classes vary considerably.\n",
    "This is close to a real world problem, therefore let's treat it as such.\n",
    "For a while we have been relying on cross-validation to check our models but for real world data,\n",
    "where we expect to have new (previously unknown) data thrown at our classifier, this is not enough.\n",
    "For this problem we will use three sets: **a training set, a validation set and a test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=50)\n",
    "len(faces.data), faces.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw this before, but just in case let's look at the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 11, figsize=(16, 3), subplot_kw={'xticks':[], 'yticks':[]})\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "first_img = [np.argmax(faces.target == x) for x in list(range(len(faces.target_names)))]\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    idx = first_img[i]\n",
    "    ax.imshow(faces.data[idx].reshape(62, 47), cmap='binary_r')\n",
    "    if i % 2:\n",
    "        ax.set_title(faces.target_names[i], fontsize=10)\n",
    "    else:\n",
    "        ax.set_xlabel(faces.target_names[i], fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's flense out a test set.\n",
    "We will not touch this set until we have a trained model with tuned hyperparameters.\n",
    "Then we will use this set to estimate the generalization of the model against\n",
    "data that it has never seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(faces.data, faces.target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model as normal, and perform cross-validation.\n",
    "We will try different kernels, let's see which kernel will perform best.\n",
    "The following is still a big problem that will take a while when run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(PCA(n_components=200, svd_solver='randomized'), SVC())\n",
    "param_dist = {\n",
    "    'svc__kernel': ['linear', 'rbf'],\n",
    "    'svc__C': sp_randint(1, 100000),\n",
    "    'svc__gamma': uniform(0.001, 10.0)\n",
    "}\n",
    "search = RandomizedSearchCV(model, param_dist, cv=5, n_iter=20)\n",
    "search.fit(xtrain, ytrain)\n",
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But plain score, be it mean accuracy or F1 does not tell us much in a multiclass problem.\n",
    "Instead `sklearn` can give us the F1 - together with its components - in a classification report.\n",
    "Moreover, we are really interested in how the model *generalizes*,\n",
    "we should test is against the *test set* which we kept untouched until now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "yfit = search.best_estimator_.predict(xtest)\n",
    "print(classification_report(ytest, yfit, target_names=faces.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a reasonable F1 score across all classes.\n",
    "There are differences where the support is poor.\n",
    "But also people who tend to have a very different face from the others have a very good F1 score.\n",
    "This is likely to mean that our classifier is doing what is supposed to do.\n",
    "Another way of visualizing this is a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "m = confusion_matrix(ytest, yfit)\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = sns.heatmap(m.T, square=True, annot=True, fmt='d', cmap='Greens',\n",
    "                 xticklabels=faces.target_names, yticklabels=faces.target_names)\n",
    "ax.set(xlabel='true label', ylabel='predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a similar picture, we can say that the generalization is good enough.\n",
    "SVMs are quite a good model for this kind of problem.\n",
    "\n",
    "Let's see how good.\n",
    "We can identify the faces on the pictures ourselves,\n",
    "we can compare that against how our model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 8, figsize=(16, 5), subplot_kw={'xticks':[], 'yticks':[]})\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.2)\n",
    "correct = yfit == ytest\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(xtest[i].reshape(62, 47), cmap='binary_r')\n",
    "    if correct[i]:\n",
    "        ax.set_title(faces.target_names[ytest[i]], fontsize=10)\n",
    "    else:\n",
    "        ax.set_title(faces.target_names[yfit[i]], fontsize=10, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [sklearn - SVM margins][1]\n",
    "- [sklearn - SVM kernels][2]\n",
    "\n",
    "[1]: http://scikit-learn.org/stable/auto_examples/svm/plot_svm_margin.html\n",
    "[2]: http://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
