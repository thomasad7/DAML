{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAML 09 - Real World Problems\n",
    "\n",
    "Michal Grochmal <michal.grochmal@city.ac.uk>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just so we have this in one place,\n",
    "let's group together all the higher level issues that concern machine learning models.\n",
    "These issues happen almost every time we want\n",
    "to use a machine learning model to solve a real world problem (i.e. not a toy problem).\n",
    "We have already discussed several of them but a list works better as a reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias, Variance, and the Size of Training Data\n",
    "\n",
    "The *bias vs variance* trade-off argues that a model that is not complex enough\n",
    "will *underfit* the data, and a model that is too complex will *overfit* the data.\n",
    "We control model complexity through model hyperparameters,\n",
    "and we can estimate a good complexity by trying several hyperparameters and\n",
    "cross-validating their performance.\n",
    "\n",
    "Yet, the model complexity found by tuning the hyperparameters is not constant\n",
    "when we add more data points.\n",
    "In other words, we can prove that a problem is solvable by sampling our data points;\n",
    "but we cannot argue that a model tuned to use certain hyperparameters\n",
    "will perform (generalize) as well on new data.\n",
    "\n",
    "If we have enough data to worry about the training time,\n",
    "we can use a **learning curve** to estimate how well our hyperparameters,\n",
    "tuned on small samples will perform on the full dataset.\n",
    "The *total variance in the dataset is directly proportional\n",
    "to the number of samples needed to explain this variance*.\n",
    "Therefore, if we train and tune our model on increasingly bigger samples of our dataset,\n",
    "the score of the model on its training data and its test set will converge.\n",
    "Once this converges we can be confident that we have found the number of samples\n",
    "needed to account for the entire variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate your Model!\n",
    "\n",
    "The difficult part of the art of machine learning is not making a model work,\n",
    "it is to prove that is works and that it will work for new data.\n",
    "Moreover, depending of the problem we may want to validate a model for different things,\n",
    "e.g. in a fraud detection model we want the recall\n",
    "of fraud data points to be the most important validation.\n",
    "\n",
    "If your model will work with new data, just cross-validating it is not enough.\n",
    "Cross-validation allows us to select the best hyperparameters,\n",
    "and gives us a good estimate of how well a model performs;\n",
    "but it does not give us an estimate of how badly our model can perform on new data,\n",
    "i.e. we do not have a generalization baseline.\n",
    "\n",
    "To estimate how our model performs against new data, we need to separate our data\n",
    "into a training and test sets and only then perform cross-validation on the training set alone.\n",
    "The resulting model's generalization can then be evaluated on the test set.\n",
    "In other words, we now have a test set, and several folds\n",
    "which are the training and validation sets.\n",
    "This ensures that the model sees only the training set during the tuning of its parameters,\n",
    "and sees only the training and validation sets during the tuning of its hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Data\n",
    "\n",
    "Machine Learning algorithms work on numbers, and assume that if a number is bigger\n",
    "it means that this number is more important.\n",
    "Yet, that is often not what we actually want.\n",
    "Most models are sensitive to the magnitude of the features,\n",
    "and scaling the features to have a similar magnitude will,\n",
    "more often than not, give better results.\n",
    "Borrowing from `sklearn` two common ways of scaling features are:\n",
    "\n",
    "- [StandardScaler][scal] subtracts the mean and then divides by the maximum (absolute)\n",
    "  to achieve mean zero and variance one for all features.\n",
    "\n",
    "- [Normalizer][norm] forces all features to have values between zero and one.\n",
    "  Neural networks often need data in this form.\n",
    "\n",
    "[scal]: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "[norm]: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles, Voting and OVO vs OVR\n",
    "\n",
    "Ensemble methods are powerful.\n",
    "Depending on how you setup the ensemble it can bestow the performance of models\n",
    "or work around limitations of certain models.\n",
    "\n",
    "A *bagging* technique (akin of, but no limited to, random forests or ada boost)\n",
    "takes several models, trains each and predicts by majority vote across all models.\n",
    "The internal models are often trained on subsets of the data, or with different\n",
    "randomization and hyperparameters.\n",
    "Bagging increases the performance and generalization of the models,\n",
    "and makes up for models which suffer from in-built overfitting (i.e. low generalization).\n",
    "\n",
    "*One vs one* (OVO) and *one vs rest* (OVR) techniques are used to allow binary classifiers\n",
    "to perform multiclass classification.\n",
    "OVO trains a classifier for every pair of classes,\n",
    "this means that a big number of binary classifiers will be trained on\n",
    "subsets of data (only the samples for the two classes).\n",
    "The OVO runs all competing classifiers and decides on the classes with most wins.\n",
    "In OVR (also called OVA, One vs All) the number of trained classifiers is the number of classes,\n",
    "each classifier is trained on the samples of one class as the positive class\n",
    "and all other samples as the negative class. \n",
    "OVR then selects the answer by picking the class with the higher score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilities versus Decision Functions\n",
    "\n",
    "Being able to explain why your model classifies things the way it does,\n",
    "may or may not be important for the problem you are solving.\n",
    "Classification (and often regression) can be performed in two ways:\n",
    "by constructing a decision function and deciding upon classes/values based\n",
    "on this function; or by assigning probabilities to each class/value and\n",
    "deciding based on the higher probability.\n",
    "\n",
    "Probabilities can be computed for every class, and therefore are easy to explain to humans,\n",
    "e.g. a misclassification between the wrong class having 51% confidence\n",
    "and the correct one having 47% confidence makes easy to argue that the model\n",
    "needs a little more tuning (but does not need a rewrite).\n",
    "With a decision function the explanation of model errors can become hairy,\n",
    "since most decision functions compute their score in very high dimensions.\n",
    "\n",
    "Some models based on decision functions are (these have a `decision_function` method in `sklearn`):\n",
    "\n",
    "- SVM\n",
    "- Logistic Regression (some regression algorithms *are* their own decision functions)\n",
    "- Neural Networks\n",
    "\n",
    "Models based on probabilities are (these have a `predict_proba` method in `sklearn`):\n",
    "\n",
    "- k Nearest Neighbors\n",
    "- Naive Bayes\n",
    "- Decision Trees and Random Forests\n",
    "\n",
    "Most classifiers that work by building a decision functions are *binary classifiers*,\n",
    "and require the use of OVO or OVR techniques to perform multilabel classification.\n",
    "Probability based classifiers perform multilabel classification by default.\n",
    "\n",
    "Dimensionality reduction has a only the *explained variance* method (in the PCA)\n",
    "that may give some insight into the model, manifold methods lack an explanation feature.\n",
    "Some clustering methods may provide probabilities but these have less\n",
    "value to explain the model since the cluster centers may be incorrect,\n",
    "instead techniques such as *silhouette scores* better visualize (and explain) a clustering model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Learning\n",
    "\n",
    "One thing we did not touch yet is the concept of *online learning*.\n",
    "This is to differ it from *offline* (or batch) learning.\n",
    "In *offline learning* we can work with all data we will ever feed the model\n",
    "with at once, i.e. we can load a dataset in memory, use cross-validation\n",
    "to tune a model over this dataset and test against a test set.\n",
    "Offline learning is incredibly common in data analysis.\n",
    "\n",
    "The problem starts when we plan to build a model (say, classifier) on top of data that\n",
    "is continuously entering or flowing through the system.\n",
    "We never have the full dataset in such a case,\n",
    "we may have all data until today but even that will not necessarily be complete.\n",
    "What we need is a model that can learn and *re-learn* from new data,\n",
    "such is an online learning model.\n",
    "\n",
    "To perform online learning we need to be capable of tuning model\n",
    "parameters to new data without looking back at the previous data.\n",
    "In other words,\n",
    "the model parameters must *represent the data seen until now* and if we change such\n",
    "a parameter slightly it will not affect the overall model too much.\n",
    "\n",
    "It turns out that most models that predict probabilities cannot be used for online learning,\n",
    "this is because these models make an estimate of a probability distribution\n",
    "and it is not possible to re-estimate a probability distribution knowing only\n",
    "the distribution and new data points.\n",
    "Well, it is possible to draw points from the distribution,\n",
    "add the new ones and re-estimate but that would still require us to know how many points to\n",
    "draw (e.g. the same number of points as in the old data).\n",
    "\n",
    "Decision functions, when cleverly parametrized, are much easier to re-tune to new data.\n",
    "A small change to a parameter in the decision function can bring the model closer\n",
    "new data with very little computational effort.\n",
    "\n",
    "But how small a change?  That answer depends on the problem.\n",
    "This small (or not so small) change is called the *learning rate* of an online learning model.\n",
    "A big learning rate will make the model forget about old data quickly,\n",
    "a very small learning rate will make the model have a lot of inertia when adapting to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do you need Online Learning?\n",
    "\n",
    "There is a huge cost in achieving actual online learning: one cannot use probabilistic models,\n",
    "must define and test a learning rate, and several models\n",
    "can only use a subset of its capabilities as online learning models\n",
    "(e.g. SVMs can only use the linear kernel because it has a tunable decision function).\n",
    "And in most problems a model does not receive data all the time.\n",
    "\n",
    "Most machine learning problems will receive data at known intervals,\n",
    "e.g. a daily snapshot of a database or a time series of the last 30 days of trades.\n",
    "This means that you can retrain your model with the new data every time you receive it.\n",
    "You may need to slightly re-tune the hyperparameters\n",
    "but the grid search should be close to the current hyperparameters,\n",
    "since the new data is unlikely to be very different from the old one.\n",
    "\n",
    "Retraining a new model at certain intervals does not need to create downtimes,\n",
    "you can automate the training and only when the training and validation finishes\n",
    "point a load balancer to the new model.\n",
    "Training a new model at every batch also allow you to test it for obvious inconsistencies.\n",
    "Since most machine learning is performed as a service:\n",
    "a trained model sits in memory waits for input and sends back predictions,\n",
    "training a model at server startup is a completely valid and often used technique.  \n",
    "\n",
    "In summary, you will only need an online learning model if either:\n",
    "\n",
    "- The entire dataset cannot fit in memory,\n",
    "  in this case you need online learning to train the model using parts of the dataset at a time.\n",
    "\n",
    "- You need very quick adaptation to new data, e.g. algorithmic trading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can I do Online Dimensionality Reduction and Clustering?\n",
    "\n",
    "Absolutely.  Moreover, it would sometimes be impossible to do online classification\n",
    "without the ability to perform online dimensionality reduction.\n",
    "The [Incremental PCA][ipca] performs PCA in an online fashion,\n",
    "it understands the idea of batch processing and updates its eigenvectors batch by batch.\n",
    "\n",
    "Using the same concept (batches) the `k-means` algorithm has a [Mini Batch k-means][mbkm] variant.\n",
    "Akin of the incremental PCA, the mini batch k-means can deal with very big datasets in an online fashion.\n",
    "In general, in `sklearn` any model that has a `partial_fit` method, can perform online learning.\n",
    "\n",
    "[ipca]: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html\n",
    "[mbkm]: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Rot\n",
    "\n",
    "An ML model is, as the name suggests,\n",
    "a mathematical technique which estimates the behavior of the real world.\n",
    "Unfortunately (fortunately?) the real world changes, and if our model\n",
    "does not change in response it will soon perform worse and worse.\n",
    "\n",
    "Of course, this is not relevant to self-closed datasets,\n",
    "as the ones seen in competitions or in toy problems.\n",
    "But most models are expected to work on real, and actual, data.\n",
    "Recent data may have new trends,\n",
    "and a performance estimate without that trend will be overoptimistic.\n",
    "In other words, the performance of your model will decrease over time if you do not update it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor your model\n",
    "\n",
    "Since the performance will decrease over time you need to know when it decreases to a point\n",
    "in which your model is not good enough anymore to perform its job.\n",
    "Even if you do not retrain your model in, say, daily batches,\n",
    "you still need to check the model's performance against new data;\n",
    "i.e. you need to cross check whether a model trained on new data\n",
    "would classify in the same manner as the current running model.\n",
    "\n",
    "Another reason to monitor your model is that you cannot test for every behavior during\n",
    "model validation (if you could you would not be using ML to solve the problem!).\n",
    "A new model, once trained on new data or an online model that has been battered with\n",
    "bad input for a while, may perform abysmally once deployed.\n",
    "In such a case you need a way of reverting to a previous model.\n",
    "For offline learning this is often easy as long as you did store previous data.\n",
    "For online learning you will need to store snapshots of your model at certain intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model\n",
    "\n",
    "First of all ask whether it makes sense to save the model.\n",
    "Often retraining on the most recent data, or a previous data snapshot at server startup\n",
    "is more convenient and even faster for some models (e.g. KNN).\n",
    "\n",
    "Since we are working with Python we can simply use Python's default way of storing\n",
    "serializable memory objects: [pickle][pick].\n",
    "The default `pickle.dumps` and `pickle.loads` work on pretty much all `sklearn` models,\n",
    "and, since `NumPy` arrays are serializable, `pickle` works as well on most other ML libraries.\n",
    "\n",
    "That said, `pickle` may result in quite bloated objects,\n",
    "this was one of the reasons [joblib][jobl] was developed.\n",
    "The `pickle` bloat is due to the fact that it converts `NumPy` arrays into lists,\n",
    "which `joblib.dumps` performs much more efficiently by storing the array as a binary lump.\n",
    "`sklearn` comes with a (possibly outdated) `joblib` copy at `sklearn.externals.joblib`.\n",
    "\n",
    "[pick]: https://docs.python.org/3/library/pickle.html\n",
    "[jobl]: https://pythonhosted.org/joblib/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Machine Learning Models\n",
    "\n",
    "We saw that we can divide machine learning problems/models in four categories:\n",
    "\n",
    "- Classification\n",
    "- Regression\n",
    "- Dimensionality Reduction\n",
    "- Clustering\n",
    "\n",
    "We also saw that the line between *classification* and *regression* is pretty thin,\n",
    "i.e. that these models only differ in how they present their outputs.\n",
    "Yet, there are even thinner divisions between other forms of machine learning techniques.\n",
    "Some other types for machine learning problems you may see out there are:\n",
    "\n",
    "- *Anomaly detection* is a binary classification problem where *normal*\n",
    "  is one of the classes and *abnormal* the other.\n",
    "  Tuning between precision and recall for these classes is often automated in a way that\n",
    "  it is easy to change an abnormal activity into a normal sample upon human inspection.\n",
    "\n",
    "- *Association Rule Learning* is a clustering problem where we use several\n",
    "  different distance measures and deterministic ways of defining clusters.\n",
    "  In other words, it is a clustering technique simplified to a level which\n",
    "  can be easily explained.\n",
    "  One such technique is hierarchical clustering.\n",
    "\n",
    "- *Reinforcement Learning* is a non-linear online learning classification/regression\n",
    "  technique with a *variable learning rate*.\n",
    "  This attempt to reuse knowledge obtained from one task into learning another task.\n",
    "  Reinforcement learning is what we often consider robot-AI, and, to be fair,\n",
    "  is quite popular in robot development.\n",
    "\n",
    "Finally there are also **genetic algorithms** and **swarm intelligence**\n",
    "which are completely different techniques.\n",
    "The mathematical foundation to the convergence of genetic algorithms\n",
    "or swarm intelligence are not developed.\n",
    "Both techniques take analogies from nature and use them to iteratively build and tune ML models.\n",
    "Experimental techniques proved that both techniques\n",
    "work but their application to new problems is tricky to develop.\n",
    "In essence, the use of these algorithms in a practical manner is viable,\n",
    "and even very successful, on specific problems.\n",
    "But the mathematical formulation of the techniques is next to non-existent.\n",
    "This may change in the following years (perhaps decades),\n",
    "if you want a not extensively explored field to work in there's your chance."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
