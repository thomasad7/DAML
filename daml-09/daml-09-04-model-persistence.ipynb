{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAML 09 - Model Persistence\n",
    "\n",
    "Michal Grochmal <michal.grochmal@city.ac.uk>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It take a moment to train a model,\n",
    "moreover since that normally mean training several models and preforming model selection.\n",
    "The reason to train a model is for it to be useful against real problems,\n",
    "often we want to use the model against data that we do not have whilst we are training it.\n",
    "Therefore we want to save the trained model for later reuse.\n",
    "\n",
    "In Python we can simply use `pickle` to dump a model to a file.\n",
    "Yet, `pickle` is particularly ineffective and slow at serializing `NumPy` arrays.\n",
    "A better option is `joblib` which uses `NumPy`'s internal memory mappings\n",
    "(memory mapped files on systems that support it) or stream compression\n",
    "to reduce the size of the file,\n",
    "the time to load it from disk and even the size of the model in memory.\n",
    "`joblib` has a `pickle` interface and we can use the below to deal with\n",
    "systems that may or may not have `joblib` installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')\n",
    "try:\n",
    "    from joblib import dump, load\n",
    "except ImportError:\n",
    "    from pickle import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some regression data and build a `RandomForestRegressor` from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 512\n",
    "X = np.sort(200 * np.random.rand(n_points, 2) - 100, axis=0)\n",
    "y = np.array(np.pi*X[:, 0] * np.sin(X[:, 1])) + 0.5 - np.random.rand(n_points)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest has a hyperparameter that we will tune using a grid search,\n",
    "the number of trees used for the regression.\n",
    "We will use cross validation to determine this hyperparameter,\n",
    "therefore the grid search will see all the data in the cross validation.\n",
    "This means that the final cross validation score may be overestimated.\n",
    "\n",
    "Before performing the grid search and the cross validation we will split\n",
    "the data into training and test sets.\n",
    "Then we run cross validation on the training set only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "grid = GridSearchCV(RandomForestRegressor(),\n",
    "                    {'n_estimators': [10, 20, 50, 100, 200]}, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a reasonable score,\n",
    "let's see how what hyperparameters we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see if we are not overestimating the generalization of this model.\n",
    "We can do this since we did keep a test set on the side until now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid.best_estimator_\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real generalization estimate is somewhere in between,\n",
    "closer to the lowest value.\n",
    "Let's have a quick visual look as well, since this model does not have too many dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], y_train, color='steelblue')\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], y_test, color='seagreen')\n",
    "n_points = 256\n",
    "pred_pi = np.linspace(-100, 100, n_points)\n",
    "pred_sin = np.linspace(-100, 100, n_points)\n",
    "gpi, gsin = np.meshgrid(pred_pi, pred_sin)\n",
    "X_pred = np.c_[gpi.ravel(), gsin.ravel()]\n",
    "y_pred = model.predict(X_pred)\n",
    "ax.plot_surface(gpi, gsin, y_pred.reshape(n_points, n_points), alpha=0.3, color='crimson')\n",
    "ax.view_init(elev=30., azim=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features were taken from the same array and are dependent on each other:\n",
    "when one feature increases the other increases too.\n",
    "This is because we sorted the values.\n",
    "As an exercise you can go back to the generation of the data an remove `np.sort`,\n",
    "the graph will look very differently but also the model will score much worse.\n",
    "\n",
    "Yet, let's say that this model is good for our purposes because we know,\n",
    "or assume, that the features are really dependent on each other.\n",
    "And, since we are confident about this model, we should save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rfc.pickle', 'wb') as f:\n",
    "    dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This just created a file called `rcf.pickle` in the current directory.\n",
    "The file is a binary (pickled) representation of the model (object) we juts used.\n",
    "We can read it back now into a different variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rfc.pickle', 'rb') as f:\n",
    "    rfc = load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can check if the model loaded back from this file is the same one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly the same test score ($R2$) as before,\n",
    "we can be quite confident that we got the same model back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats\n",
    "\n",
    "The interoperability between different versions of Python's `pickle` module is poor.\n",
    "In other words, exchanging models between Python 2 and Python 3 projects\n",
    "through `pickle` is a very bad idea.\n",
    "That said, Python 2 is reaching EOL in 2020, where this concern should end.\n",
    "\n",
    "More advanced uses of `joblib`, e.g. gzipped files, do not keep full compatibility to `pickle`.\n",
    "It isn't anything that cannot be worked around by uncompressing a file before unpickling\n",
    "but one needs to be consistent on how to treat persisted models,\n",
    "the files contain very little metadata on how to load them back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `joblib`\n",
    "\n",
    "Is much more extensive than just persisting models.\n",
    "The main aim of the library is to automate several processes in a machine learning\n",
    "pipeline by memoizing intermediate results to disk, as extended pickle.\n",
    "Two things that the library specializes are:\n",
    "\n",
    "- Memoize results of huge computations on disk,\n",
    "  so one does not necessarily need to re-run the entire processing if something goes wrong.\n",
    "\n",
    "- Memory map `NumPy` arrays during parallel processing,\n",
    "  allowing separate processes to share memory and preventing copying huge arrays between processes.\n",
    "\n",
    "For training big models on a single machine, `joblib` helps a good deal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Should I always persist my model?\n",
    "\n",
    "Not necessarily.\n",
    "An ML model will wander off course over time,\n",
    "i.e. the real world will change in a way that the model do not reflect it anymore.\n",
    "Often one needs to retrain a model periodically for it to be useful.\n",
    "One would collect the data that is thrown at the model and construct a new training\n",
    "set by mixing the new data with the old training set.\n",
    "\n",
    "A case where retraining a model makes sense are web services.\n",
    "A web service will run several models of the same kind, and will predict values for user requests.\n",
    "The service runs several models for load balancing,\n",
    "i.e. even when one of the machines goes down the service can still respond to users.\n",
    "These machines run non-stop but sometimes need to be taken down for maintenance:\n",
    "a machine is taken down, updated, restarted, tested and only then placed back into the load balancing.\n",
    "Since the machine restart may take a while,\n",
    "re-training the model that is part of the service during this restart is a good idea.\n",
    "The re-trained model knows about new training data and will be (hopefully) better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
