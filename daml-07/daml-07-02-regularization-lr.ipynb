{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAML 07 - Regularization and Linear Regression\n",
    "\n",
    "Michal Grochmal <michal.grochmal@city.ac.uk>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression is different from classification in that it tries to predict continuous values.\n",
    "In other words, we are predicting a value instead of classes or labels.\n",
    "Regression is somewhat older then classification, and stems from experimental science,\n",
    "which collected data in the attempt to find mathematical representations\n",
    "(functions) of natural phenomena.\n",
    "During that period regression was (and often still is) called *function approximation*.\n",
    "The simplest form of regression is the *least squares* technique.\n",
    "\n",
    "Let's say that we performed an collected experimental data.\n",
    "This data is likely to be an insight into a process about which we want to learn.\n",
    "Yet, the data is incomplete and ridden with experimental error,\n",
    "making it hard to recreate the process that was measured.\n",
    "In the simplest case we'd like to find a line $y = w_1x + w_0$ which\n",
    "*we assume* would be a good approximation of the process.\n",
    "\n",
    "Note: There are several notations used for linear regression or function approximation.\n",
    "Some literature will draw the line equation as $f(x) = ax + b$, others as $y = a_0 + a_1x$,\n",
    "or a combination of these notations.\n",
    "In Machine Learning most people prefer to use $w$ to denote the parameters,\n",
    "a mnemonic to *weight*.  In this notation we write the line equation as $y = w_1x + w_0$.\n",
    "\n",
    "`sklearn`'s `LinearRegression`, by default, performs a least squares approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Road Trip\n",
    "\n",
    "As a simple example we can **estimate the mean speed** on a road curve by **measuring\n",
    "the position** of every car starting from the entry into the curve $(t = 0)$.\n",
    "Let's say we got the following measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 10*np.random.rand(60)\n",
    "pos1 = 7 + t*3 + 2*np.random.randn(*t.shape)\n",
    "pos2 = 7 + t*3 + 2*np.random.randn(*t.shape)\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot(t, pos1, 'o', color='crimson', alpha=0.7)\n",
    "ax.plot(t, pos2, 'o', color='steelblue', alpha=0.7)\n",
    "ax.set_xlabel('time (s)')\n",
    "ax.set_ylabel('position (m)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least squares attempts to minimize the error (residual) across all points:\n",
    "$$r = \\sum_{i=1}^{N} (f(x_i) - y_i)^2 = \\sum (f(\\vec{X}) - \\vec{y})^2$$\n",
    "\n",
    "We can say that we are minimizing the error of the point to a function of the form:\n",
    "\n",
    "$$y = w_1x + w_0$$\n",
    "\n",
    "The minimization, in two dimensions, can be understood as (here $X$, $w$ and $y$ are vectors):\n",
    "\n",
    "$$\\min_{w} \\|Xw - y\\|$$\n",
    "\n",
    "For the case of a line $y = w_1x + w_0$ we search for the parameters $w_0$ and $w_1$,\n",
    "and these can be found analytically.\n",
    "There are several ways of getting to these equations,\n",
    "one of them is to walk the function formed by the residuals for all values of $w_0$ and $w_1$.\n",
    "The surface of the parameters $w_0$ and $w_1$ is concave,\n",
    "therefore exist a single point where the gradient is exactly zero.\n",
    "Form this point we get.\n",
    "\n",
    "$$\n",
    "w_0 = \\frac{\\overline{y} \\sum_{i=0}^{n} x_i^2 - \\overline{x} \\sum_{i=0}^{n} x_iy_i}\n",
    "{\\sum_{i=0}^{n} x_i^2 - n \\overline{x}^2}\\\\\n",
    "w_1 = \\frac{\\sum_{i=0}^{n} x_iy_i - n \\overline{x} \\overline{y}}\n",
    "{\\sum_{i=0}^{n} x_i^2 - n \\overline{x}^2}\n",
    "$$\n",
    "\n",
    "In our case $a_1$ is the *mean speed* we are after.  `sklearn` will do the calculation for us.\n",
    "The linear regression in `sklearn` produces the resulting parameters as `intercept_` containing $w_0$\n",
    "and `coef_` containing $w_1, w_2, ...$ (for example, in the case where we have more dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = np.hstack([pos1, pos2])\n",
    "t_points = np.vstack([t[:, np.newaxis], t[:, np.newaxis]])\n",
    "model = LinearRegression()\n",
    "model.fit(t_points, pos)\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.scatter(t, pos1, color='crimson', alpha=0.7)\n",
    "ax.scatter(t, pos2, color='steelblue', alpha=0.7)\n",
    "ax.plot(xfit, yfit, color='navy')\n",
    "ax.set_xlabel('time (s)')\n",
    "ax.set_ylabel('position (m)')\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to **measure the mean speed** (assuming that we can compare positions very quickly).\n",
    "But what if we want the **typical speed at each point** across more than just a single curve?\n",
    "Let's generate some data that is akin of:\n",
    "\n",
    "1.  Slowing down before turn.\n",
    "2.  Gaining speed slowly in the turn.\n",
    "3.  Gaining speed faster after exiting the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 20*np.random.rand(300)\n",
    "spd = 7 + 0.2*(t-7.5)**2 + 2*np.random.randn(*t.shape)\n",
    "#spd = 18 + -3*t + 0.2*t**2 + 2*np.random.randn(*t.shape)\n",
    "fig, ax = plt.subplots(figsize=(14, 9))\n",
    "ax.plot(t, spd, 'o', color='crimson')\n",
    "ax.set_xlabel('time (s)')\n",
    "ax.set_ylabel('speed (km/h)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That kind of looks like a function akin of:\n",
    "\n",
    "$$y = w_2x^2 + w_1x + w_0$$\n",
    "\n",
    "The traditional (read: non-computational) method to solve this problem is to get rid of $x^2$\n",
    "by applying a logarithmic transformation to the axes, and then apply the linear\n",
    "regression as normal since the graph would be a line.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 9))\n",
    "ax.semilogy(t, spd, 'o', color='crimson')\n",
    "ax.set_xlabel('time (s)')\n",
    "ax.set_ylabel('ln(speed) (km/h)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now divide our data at, say, $t = 7.5$ and generate two linear regressions,\n",
    "one for each side.  But there are better ways!\n",
    "\n",
    "Since we can easily operate in more than two dimensions (time and speed in our case)\n",
    "let's build this same model in three dimensions: time, speed *and speed squared*.\n",
    "`sklearn` provides us with `PolynomialFeatures` preprocessor for such cases.\n",
    "\n",
    "In more dimensions we will search for more coefficients.\n",
    "The general least squares equation can compe with as many dimensions as we need:\n",
    "\n",
    "$$\\min_{w_1, \\dots, w_{d - 1}} \\|Xw_j - y\\|$$\n",
    "\n",
    "Which is analogous to:\n",
    "\n",
    "$$f(x_0, x_1, \\dots, x_i) = w_0x_0 + w_1x_1 + \\dots + w_ix_i + w_{i+1}$$\n",
    "\n",
    "Where a single coefficient is multiplies by each dimension (each axis).\n",
    "But nothing prevents us from doing the following,\n",
    "i.e. use the same dimension twice, with a tweak:\n",
    "\n",
    "$$f(x) = w_0x + w_1x + w_2x^2$$\n",
    "\n",
    "This is still a linear model since neither $w$ parameter multiplies each other.\n",
    "`sklearn`'s `PolynomialFeatures` can transform the data for us\n",
    "(instead of us doing it by hand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pol = PolynomialFeatures(degree=2)\n",
    "pol.fit_transform(spd[:, np.newaxis])[:10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good, we have a matrix of parameters for $x^0, x^1 x^2$,\n",
    "now we can fit a line to that and then multiply by these parameters\n",
    "again when we `perdict` the result.\n",
    "Remembering to multiply things back when predicting is something that `sklearn` can do for us.\n",
    "We use a **pipeline** which joins one - or more - preprocessors with an estimator,\n",
    "in this case we join the polynomial features with the linear regression.\n",
    "This is what a Polynomial Regression is below the hood.\n",
    "\n",
    "We now have one *hyperparameter* in our model, the degree of the polynomial.\n",
    "That said, we are rather confident that we are after a 2nd degree polynomial,\n",
    "therefore we probably know the best value for that hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
    "model.fit(t[:, np.newaxis], spd[:, np.newaxis])\n",
    "xfit = np.linspace(0, 20, 2000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "fig, ax = plt.subplots(figsize=(14, 9))\n",
    "ax.scatter(t, spd, color='crimson', alpha=0.7)\n",
    "ax.plot(xfit, yfit, color='navy')\n",
    "ax.set_xlabel('time (s)')\n",
    "ax.set_ylabel('speed (km/h)')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_steps.linearregression.intercept_, model.named_steps.linearregression.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are confident, by-eye, that our estimation is good.\n",
    "Still, let's be careful and evaluate how our model performs on unknown data.\n",
    "\n",
    "Each model type in `sklearn` contains a `.score` method,\n",
    "which contains a scoring method appropriate for a common model of that type.\n",
    "On classification models it will contain the mean accuracy or F1 score.\n",
    "For the linear regression the scoring is the [Coefficient of Determination (R2)][r2score]:\n",
    "the residual error after regression divided by the total regression error\n",
    "(it is always between 0 and 1).\n",
    "\n",
    "[r2score]: https://en.wikipedia.org/wiki/Coefficient_of_determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "t1, t2, s1, s2 = train_test_split(t, spd, train_size=0.7, test_size=0.3)\n",
    "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
    "model.fit(t1[:, np.newaxis], s1[:, np.newaxis])\n",
    "model.score(t2[:, np.newaxis], s2[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "As we saw before, the score varies depending on how the training and test sets are selected.\n",
    "Therefore, in order to get a good sense on how the model performs,\n",
    "we should repeat the train-test split and perform the scoring at each split.\n",
    "\n",
    "Yet, `sklearn` has automated that for us  with `cross_val_score`, a way of splitting data\n",
    "into training and test sets in several part and then scoring each pair.\n",
    "This concept is called **cross-validation** and is prominent in all of machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
    "cross_val_score(model, t[:, np.newaxis], spd[:, np.newaxis], cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can reasonably say that we are confident in our model.\n",
    "In other words, thanks to the cross-validation scores,\n",
    "we are confident that our model can reasonably predict values for new input data.\n",
    "In machine learning we often say that our model can **generalize** well\n",
    "to previously unknown data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Road Trip\n",
    "\n",
    "Let's move forward: can we estimate the speed at each point on a longer strip of a road?\n",
    "For example, some 30km of a road with turns and inclines.\n",
    "\n",
    "For our simulation it is interesting to note that most problems in the real world are not linear,\n",
    "they are either exponential or periodic.  Why?  Something, something complexity theory.\n",
    "Anyway, speed on a road is a periodic problem, one speeds up and down in response\n",
    "to the shape of the road he drives on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 30*np.random.rand(200)\n",
    "spd = 13*np.sin(t/2) + 3.7*np.cos(t/2+7) + 3*t + 0.1*(t-10)**2 - 3*(t-3) + 7 + 2.3*np.random.randn(*t.shape)\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "ax.plot(t, spd, 'o', color='crimson')\n",
    "ax.set(xlabel='time (s)', ylabel='speed (km/h)', xlim=(-2, 32), ylim=(-10, 70));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is pretty difficult to figure out what polynomial degree we need for this fit.\n",
    "But let's try a guess, degree 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(degree=5), LinearRegression())\n",
    "model.fit(t[:, np.newaxis], spd[:, np.newaxis])\n",
    "xfit = np.linspace(0, 30, 3000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "ax.scatter(t, spd, color='crimson', alpha=0.7)\n",
    "ax.plot(xfit, yfit, color='navy')\n",
    "ax.set(xlabel='time (s)', ylabel='speed (km/h)', xlim=(-2, 32), ylim=(-10, 70))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch, that went pretty badly.  This is a case where out model **underfits** the data,\n",
    "i.e. our model has not enough complexity to model the complexity we see.\n",
    "We can also say that our model has too much **bias** about how the data looks.\n",
    "\n",
    "Let us try with a big degree, e.g. 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(degree=100), LinearRegression())\n",
    "model.fit(t[:, np.newaxis], spd[:, np.newaxis])\n",
    "xfit = np.linspace(0, 30, 3000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "ax.scatter(t, spd, color='crimson', alpha=0.7)\n",
    "ax.plot(xfit, yfit, color='navy')\n",
    "ax.set(xlabel='time (s)', ylabel='speed (km/h)', xlim=(-2, 32), ylim=(-10, 70))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That ain't good either.\n",
    "On the left hand side we passed the point where we can bend\n",
    "the polynomial and our parameters mess with each other.\n",
    "On the right hand side the polynomial **overfits** the data;\n",
    "or we say that the model has too much **variance**.\n",
    "\n",
    "There is more than one way to solve this problem.\n",
    "Let's see a handful of possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias versus Variance - Model Selection\n",
    "\n",
    "To solve the right hand side problem we need to tune of *hyperparameter*,\n",
    "the degree of our polynomial.\n",
    "Until now we have been guessing and doing this by hand but trying all values between 5 and 100\n",
    "by hand does not seem like a good way of spending an afternoon.\n",
    "Instead `sklearn` can automate this for us.\n",
    "\n",
    "We will train a model for every degree between 5 and 100 and evaluate,\n",
    "by cross-validation, which model performs better.\n",
    "`sklearn` provides us with a grid search algorithm,\n",
    "which will perform the training and cross-validating of our model\n",
    "for all hyperparameter values given to it.\n",
    "This may already sort our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(degree=5), LinearRegression())\n",
    "grid = GridSearchCV(model,\n",
    "                    {'polynomialfeatures__degree': list(range(5, 101))},\n",
    "                    cv=5)\n",
    "grid.fit(t[:, np.newaxis], spd[:, np.newaxis])\n",
    "best = grid.best_estimator_\n",
    "xfit = np.linspace(0, 30, 3000)\n",
    "yfit = best.predict(xfit[:, np.newaxis])\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "ax.scatter(t, spd, color='crimson', alpha=0.7)\n",
    "ax.plot(xfit, yfit, color='navy')\n",
    "ax.set(xlabel='time (s)', ylabel='speed (km/h)', xlim=(-2, 32), ylim=(-10, 70))\n",
    "best, best.named_steps.linearregression.intercept_, best.named_steps.linearregression.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we should have a look at the `R2` of the best estimator we have built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often simple *hyperparameter* tuning is enough to solve even complex problem.\n",
    "Yet, the problem we saw on the left hand side was a problem of dependence between dimensions,\n",
    "and sometimes it cannot be easily solved.  We shall look at more techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "The problem we see on the left hand side is that the linear regression\n",
    "assumes that the **variables are independent of each other**,\n",
    "yet $t^{16}$ is very dependent on $t^2$.\n",
    "To reduce this dependence a few techniques exist,these build a **cost function** in which\n",
    "having many coefficients that are not zero introduces a lot of error.\n",
    "This is called **regularization**, for **parametric** techniques we have:\n",
    "\n",
    "#### Ridge (often called `L2`)\n",
    "\n",
    "$$\\min_{w_1, \\dots, w_{d - 1}} \\|Xw_j - y\\|_2^2 + \\alpha\\|w_j\\|_2^2$$\n",
    "\n",
    "#### Lasso (often called `L1`)\n",
    "\n",
    "$$\\min_{w_1, \\dots, w_{d - 1}} \\frac{1}{2\\cdot N_{samples}} \\|Xw_j - y\\|_2^2 + \\alpha\\|w_j\\|_1$$\n",
    "\n",
    "#### Elastic Net\n",
    "\n",
    "$$\n",
    "\\gamma\\texttt{Lasso} + (1 - \\gamma)\\texttt{Ridge}\n",
    "\\space = \\min_{w_1, \\dots, w_{d - 1}} \\frac{1}{2\\cdot N_{samples}} \\|Xw_j - y\\|_2^2\n",
    "\\space + \\gamma\\alpha\\|w_j\\|_1 + \\frac{1 - \\gamma}{2}\\alpha\\|w_j\\|_2^2\n",
    "$$\n",
    "\n",
    "Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "#model = make_pipeline(PolynomialFeatures(degree=100), Ridge(alpha=10.0, tol=0.01, max_iter=300000))\n",
    "model = make_pipeline(PolynomialFeatures(degree=100), Lasso(alpha=10.0, tol=0.01, max_iter=300000))\n",
    "model.fit(t[:, np.newaxis], spd[:, np.newaxis])\n",
    "xfit = np.linspace(0, 30, 3000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "ax.scatter(t, spd, color='crimson', alpha=0.7)\n",
    "ax.plot(xfit, yfit, color='navy')\n",
    "ax.set(xlabel='time (s)', ylabel='speed (km/h)', xlim=(-2, 32), ylim=(-10, 70))\n",
    "model, model.named_steps.lasso.intercept_, model.named_steps.lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the fact that we know that a polynomial of degree 100 heavily *overfits* the data,\n",
    "and, moreover, its parameters start to influence each other, we still managed to find a good fit.\n",
    "This is because, thanks to the regularization, the parameters that were heavily dependent\n",
    "on each other were forced to be very close to zero.\n",
    "\n",
    "This is good but likely this is still not the best solution we can achieve.\n",
    "Now we got another *hyperparameter* to tune: the *alpha* of the ridge regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection - Again\n",
    "\n",
    "We did this before but now we have two hyperparameters to tune:\n",
    "polynomial *degree* and ridge *alpha*.\n",
    "The grid search uses a double underscore (`__`) to indicate a hyperparameter\n",
    "(argument to model constructor).\n",
    "In a pipeline the - all lowercase - name of the model, followed by the double\n",
    "underscore, followed by the hyperparameter values to try;\n",
    "performs the training and cross-validation across *all combinations* in the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "# clear output of warnings, we will root out bad fits with crossvalidation\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(degree=5), Ridge(alpha=1.0, tol=0.1, max_iter=3000))\n",
    "grid = GridSearchCV(model,\n",
    "                    {'polynomialfeatures__degree': list(range(5, 21)),\n",
    "                     'ridge__alpha': [0.1, 0.5, 1, 2, 3, 5, 10, 20, 50, 100, 200, 300, 500]},\n",
    "                    cv=5)\n",
    "grid.fit(t[:, np.newaxis], spd[:, np.newaxis])\n",
    "grid.best_estimator_, grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have a quite good estimator.  Let's see how it plots over the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(0, 30, 3000)\n",
    "yfit = grid.best_estimator_.predict(xfit[:, np.newaxis])\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "ax.scatter(t, spd, color='crimson', alpha=0.7)\n",
    "ax.plot(xfit, yfit, color='navy')\n",
    "ax.set(xlabel='time (s)', ylabel='speed (km/h)', xlim=(-2, 32), ylim=(-10, 70));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take home message - Generalization\n",
    "\n",
    "We managed to estimate a quite complex function.  But most importantly, we learned that we\n",
    "have an arsenal of machine learning automation in `sklearn`, cross-validation and grid-search\n",
    "are common to almost every machine learning problem, no matter the model used.\n",
    "\n",
    "Also, we saw a model that *underfits* - has high *bias* - and a model that *overfits* - has high *variance*.\n",
    "Models with either too high bias or variance will **generalize** poorly.\n",
    "In other words, when we perform model selection we are searching for the model that best *generalizes*.\n",
    "And we can only say that a model generalizes well to new data if we can prove that\n",
    "the model does not underfit or overfit the data independently on how we test it.\n",
    "\n",
    "Note: Just because we got the model that best generalizes across a huge grid of hyperparameters\n",
    "and across a vast cross-validation, it does not mean we have the best model that we can get.\n",
    "We have the model that can best generalize under the assumptions we make when building it.\n",
    "For example, trying a specific ML technique is an assumption about the data.\n",
    "Comparing different ML techniques on the same data means comparing the best model\n",
    "(the one that best generalizes) built with each technique on this data."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
