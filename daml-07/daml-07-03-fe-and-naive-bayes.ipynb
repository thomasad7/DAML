{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAML 07 - Feature Engineering and Naive Bayes\n",
    "\n",
    "Michal Grochmal <michal.grochmal@city.ac.uk>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning deals only with numeric data\n",
    "but not all data in the world is numeric.\n",
    "Two examples of non-numerical data that is meaningful for learning about data are:\n",
    "categorical features and plain text (e.g. product reviews).\n",
    "There are tricks that allow us to deal with non-numerical data,\n",
    "these tricks are part of *feature engineering*.\n",
    "\n",
    "For a start let's import a handful of things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Dealing with non-numerical data is only a part of feature engineering,\n",
    "although is often the most common application that is called by this name.\n",
    "Actually feature engineering is not a collection of techniques\n",
    "but a generic name to define tasks performed around input data to our model.\n",
    "These include:\n",
    "\n",
    "- Modifying existing features - e.g. scaling\n",
    "- Selecting only a subset of features - e.g. removing highly correlated features\n",
    "- Building new features from existing ones - e.g. squaring features to get only positive values\n",
    "- Encoding features in a different representation - e.g. one-hot-encoding\n",
    "- Learning new features from data - e.g. huge neural networks fed with lots of data\n",
    "\n",
    "The last example requires really huge amounts of data,\n",
    "hundreds of millions of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data\n",
    "\n",
    "When we deal with the proper names of things or people we are most often dealing\n",
    "with categorical data.  One example of such would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = ['Northern Ireland', 'Scotland', 'Wales', 'England', 'Isle of Man']\n",
    "area = np.array([14130, 77933, 20779, 130279, 572])\n",
    "population2011 = np.array([1810863, 5313600, 3063456, 53012456, 83314])\n",
    "data = pd.DataFrame({'area': area,\n",
    "                     'country': country,\n",
    "                     'population': population2011})\n",
    "data.country = data.country.astype('category')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we forced the country to have a categorical type.\n",
    "In `pandas` that is a way of assigning numbers to a column,\n",
    "these numbers then reference a set of categorical labels.\n",
    "\n",
    "This also means that the data now is completely numerical.\n",
    "i.e. we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(pd.DataFrame({'country': data.country.cat.codes}),\n",
    "         data[['area', 'population']], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet, that is *not* enough.\n",
    "Numerical values have an order, therefore we can test for inequality.\n",
    "Based on the data above we can say that:\n",
    "\n",
    "$$\\texttt{Isle of Man} < \\texttt{Wales}$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\texttt{Scotland} > \\texttt{England}$$\n",
    "\n",
    "Unfortunately, apart from their use in rugby jokes, these inequalities are rather useless.\n",
    "Moreover these inequalities are likely to confuse an ML algorithm.\n",
    "Instead we need to encode the data into a form called **one-hot-encoding**.\n",
    "Each sample has several features built from the categorical feature\n",
    "but only one of the columns contain a one, all other columns contain zeros.\n",
    "\n",
    "`pandas`' `get_dummies` exists for this exact purpose,\n",
    "to build a one-hot-encoding from a categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(data, prefix_sep='=')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is something that we can feed into an ML technique without worrying about confusing it.\n",
    "That said, this representation can use huge amounts of memory if there is a big number of features.\n",
    "To alleviate the memory problem `sklearn` can perform one-hot-encoding on sparse matrices (from `scipy`),\n",
    "this way we only need to store the ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textual Data\n",
    "\n",
    "Plain, unorganized, text data present different challenges to transform into a numeric representation.\n",
    "For a start we cannot just one-hot-encode words because they may appear more than once in each sample.\n",
    "We could encode the presence of words in each sample but when distinguishing between samples\n",
    "certain words are certainly more important than others, e.g. we can safely assume that\n",
    "the word \"the\" will appear in almost every sample.\n",
    "\n",
    "Search engine research produced an elegant technique to encode words in plain test:\n",
    "*Term Frequency by Inverse Document Frequency* (TF-IDF).\n",
    "Each word in a sample is represented by the count of this word divided by the frequency\n",
    "of this same word across all samples.\n",
    "Each sample has a feature per each word in the entire corpus (all samples),\n",
    "all words that are not present in the sample are encoded as zeros.\n",
    "\n",
    "This produces a huge sparse matrix representation of the data.\n",
    "We can try it out with samples from *newsgroups*.\n",
    "And since newsgroups are aggregated by topic we will try to classify the samples into topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = [\n",
    "    'comp.graphics',\n",
    "    'comp.windows.x',\n",
    "    'misc.forsale',\n",
    "    'rec.autos',\n",
    "    'rec.sport.hockey',\n",
    "    'sci.med',\n",
    "    'sci.space',\n",
    "    'soc.religion.christian',\n",
    "    'talk.politics.misc',\n",
    "]\n",
    "train = fetch_20newsgroups(categories=newsgroups, subset='train')\n",
    "test = fetch_20newsgroups(categories=newsgroups, subset='test')\n",
    "train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is already divided into train and test sets.\n",
    "We will make a pipeline of a TF-IDF preprocessor and a Naive Bayes classifier.\n",
    "The Naive Bayes classifier is a very simple **non-parametric** technique that just attempt\n",
    "to build (hyper)spherical probabilistic generators around the center of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Naive Bayes has no specific parameter and no tunable hyperparameters,\n",
    "it is a very good technique for a classification baseline.\n",
    "Here we use a multinomial Naive Bayes classifier because we have many features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "model.fit(train.data, train.target)\n",
    "labels = model.predict(test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have lots of classes (9 different newsgroup topics) a single score may\n",
    "not be the best approach to understand how our model works.\n",
    "Instead we will build a confusion matrix, which will give us\n",
    "true positives, false positives, true negatives and false negatives for each class.\n",
    "We can then evaluate which classes the model is better at identifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "mat = confusion_matrix(test.target, labels)\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = sns.heatmap(mat.T, square=True, annot=True, fmt='d', cmap='viridis',\n",
    "                 xticklabels=train.target_names, yticklabels=train.target_names)\n",
    "ax.set_xlabel('true label')\n",
    "ax.set_ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The worst result is across *religion* and *politics*.\n",
    "No surprises there, these topics get intermingled in the real world too.\n",
    "\n",
    "That said, with a very simple classifier and some data encoding we have built\n",
    "a model that can tell us the topic of a sentence.\n",
    "We can see it in action with a small helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_chat(sentence):\n",
    "    predicted = model.predict([sentence])\n",
    "    return train.target_names[predicted[0]]\n",
    "\n",
    "\n",
    "print('TUNING', predict_chat(\"I've added a new set of cyllinders, now I'm not even making 10 miles per galon\"))\n",
    "print('BALL', predict_chat('The ball never went even close to the goal'))\n",
    "print('BUTTON', predict_chat(\"Dude, I'm telling you, there is no such button on my screen\"))\n",
    "print('WIFE', predict_chat('My wife went shopping in the morning, has not come back yet'))\n",
    "print('PRESCRIPTION', predict_chat('Got my prescription rejected at the pharmacy'))\n",
    "print('APOLLO', predict_chat('No one ever landed on the moon, it was all a farse'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that all this is doing is checking the word frequency probabilities,\n",
    "this is a rather amazing result for a such a simple algorithm.\n",
    "\n",
    "And we can still see the problems with *religion* and *politics* in the predictions.\n",
    "This problem happens because these two topics use lots of *stop words*,\n",
    "i.e. words that are commonly used in sentence construction.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_chat('the what where')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we remove the stop words from the data representation we should\n",
    "get a better separation between religion and politics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Term frequency and weighting - Introduction to Information Retrieval][1]\n",
    "- [Weighting Schemes - Introduction to Information Retrieval][2]\n",
    "\n",
    "[1]: https://nlp.stanford.edu/IR-book/html/htmledition/term-frequency-and-weighting-1.html \"TF-IDF\"\n",
    "[2]: https://nlp.stanford.edu/IR-book/html/htmledition/document-and-query-weighting-schemes-1.html \"weighting\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
