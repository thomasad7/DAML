{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAML 10 - Online Learning\n",
    "\n",
    "Michal Grochmal <michal.grochmal@city.ac.uk>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different models that support online learning but the technique to change\n",
    "the model parameters during the learning is (almost) always some variant on top of\n",
    "Gradient Descent (GD).\n",
    "The GD is a technique which attempts to find\n",
    "a minimal model error by walking through the function of model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mpl_toolkits import mplot3d\n",
    "plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Imagine a model with two parameters (e.g. a linear regression in 2 dimensions),\n",
    "for each combination of parameters we have some model error (misclassification\n",
    "or distance to regression line).\n",
    "The shape of this function is determined by the data to which we are fitting the model.\n",
    "\n",
    "In offline/single batch learning we fit a model by repeating techniques that find an almost\n",
    "optimal value for the parameters based on a training set.\n",
    "We use cross-validation and a test set to achieve some degree of generalization.\n",
    "If we try to fit this same model to a different set of data we would repeat the entire\n",
    "technique and end at a reasonable solution for the new dataset.\n",
    "\n",
    "With online learning we try to optimize the model parameters in a slightly different way.\n",
    "We initialize the parameters at random and calculate the gradient over this model error\n",
    "function.\n",
    "The gradient is:\n",
    "\n",
    "$$\n",
    "\\nabla E = \\frac{\\partial E}{\\partial p_1}\\hat{\\imath} + \\frac{\\partial E}{\\partial p_2}\\hat{\\jmath} + \\ldots\n",
    "$$\n",
    "\n",
    "i.e. it is the partial derivative against each function parameter, in two dimensions\n",
    "it is against two parameters only ($\\hat{\\imath}$ and $\\hat{\\jmath}$ are *versors*).\n",
    "\n",
    "The gradient tells us how a function varies around the current model parameters,\n",
    "and therefore it tells us in which direction lie a better (lower model error) parameters.\n",
    "Yet, it does not tell us *how far away these parameter lie*.\n",
    "The *learning rate*, in online learning, is the distance that we will move in the direction\n",
    "of lower model error.\n",
    "And after we move to those parameters we will look at the gradient\n",
    "again and repeat the procedure, until convergence (or maximum iterations) is reached.\n",
    "\n",
    "By default `sklearn` uses a learning rate (`eta0`) that is reduced at each iteration,\n",
    "this allows for a technique called *Stochastic Gradient Descent* (SDG).\n",
    "In SDG only some of the samples are used at each time to determine the gradient.\n",
    "The decreasing learning rate allows for convergence despite the fact that not all samples\n",
    "are used to calculate the gradient.\n",
    "The default learning rate starts at $1/\\alpha$,\n",
    "where $\\alpha$ is the constant multiplying the regularization term.\n",
    "\n",
    "Let's try to visualize this on a surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 100)\n",
    "y = np.linspace(0, 10, 100)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "z = 3*np.sin(xx) - 1 + np.sin(xx + 6) + np.cos(yy) + np.cos(yy - 0.5) + 0.6*x\n",
    "z = 1 - z/20 - 0.5\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(xx, yy, z, cmap='spring')\n",
    "ax.set_zlim(0, 1)\n",
    "ax.set_zlabel('Model Error', fontsize=20, labelpad=10)\n",
    "ax.set_xlabel('Model Parameter', fontsize=20, labelpad=15)\n",
    "ax.set_ylabel('Model Parameter', fontsize=20, labelpad=15)\n",
    "ax.view_init(elev=30., azim=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a real problem the surface will be a high dimensional hyperplane,\n",
    "since most machine learning models work in very high dimensions.\n",
    "Yet, the same technique works on any number of dimensions.\n",
    "\n",
    "The function we try to optimize is called a *cost function*,\n",
    "and in several cases a cost function may be different from the\n",
    "actual function we are fitting the model with.\n",
    "We do not really care about all components of the gradient vector.\n",
    "Each parameter in the model is a dimension of the cost function,\n",
    "and the component of the gradient vector in that same dimension is an indicator of how\n",
    "the error changes if we change this specific parameter/weight.\n",
    "In other words the ratio of change (derivative) between error and parameter\n",
    "tells us in which direction a specific parameter should be updated to reach a smaller error.\n",
    "It is often written that instead of the gradient, what is used are\n",
    "*directed derivatives* in the direction of every parameter/weight.\n",
    "\n",
    "$$\n",
    "\\nabla E \\times \\vec{u} = \\| \\nabla E \\|_2 \\| u \\|_2 \\cos \\theta\n",
    "$$\n",
    "\n",
    "For example the `Ridge` regression added an extra term to the function\n",
    "we tried to optimize, and that extra term allowed us for a better fit.\n",
    "The gradient descent optimizes the cost function but the model itself\n",
    "predicts values based on the actual fitted function (from the model\n",
    "parameters only).\n",
    "In other words, we have the function (e.g. classification) we are trying to optimize,\n",
    "and to find that we build and optimize a cost function,\n",
    "which is a completely different function.\n",
    "\n",
    "In `sklearn` we have `SDGClassifier` which will perform the technique\n",
    "above to achieve online learning on top of linear SVMs, logistic regression, or a perceptron.\n",
    "The `SDGRegressor` performs a linear regression as online learning.\n",
    "Note that this means that we can only find solutions to problems\n",
    "that can be approximated linearly.\n",
    "For non-linear online learning we need neural networks (which we will see soon)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ozone Dataset\n",
    "\n",
    "For a change let's take on a dataset that is not present inside `sklearn`.\n",
    "The ozone dataset are meteorological instrument sensor measures on several days\n",
    "(1998 to 2004) and whether these days had high ozone concentration.\n",
    "This allows for the prediction of high ozone days.\n",
    "\n",
    "But first we need to write a function to actually retrieve the dataset,\n",
    "we will duplicate the `sklearn` convention for dataset loading and construct\n",
    "our `load_ozone` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "\n",
    "def load_ozone():\n",
    "    ozone_dir = 'uci_ozone'\n",
    "    data_dir = datasets.get_data_home()\n",
    "    data_path = os.path.join(data_dir, ozone_dir, 'onehr.data')\n",
    "    descr_path = os.path.join(data_dir, ozone_dir, 'onehr.names')\n",
    "    ozone_data = 'https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data'\n",
    "    ozone_descr = 'https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.names'\n",
    "    os.makedirs(os.path.join(data_dir, ozone_dir), exist_ok=True)\n",
    "    columns = [\n",
    "        'WSR0', 'WSR1', 'WSR2', 'WSR3', 'WSR4', 'WSR5', 'WSR6', 'WSR7', 'WSR8', 'WSR9',\n",
    "        'WSR10', 'WSR11', 'WSR12', 'WSR13', 'WSR14', 'WSR15', 'WSR16', 'WSR17', 'WSR18', 'WSR19',\n",
    "        'WSR20', 'WSR21', 'WSR22', 'WSR23', 'WSR_PK', 'WSR_AV',\n",
    "        'T0', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9',\n",
    "        'T10', 'T11', 'T12', 'T13', 'T14', 'T15', 'T16', 'T17', 'T18', 'T19',\n",
    "        'T20', 'T21', 'T22', 'T23', 'T_PK', 'T_AV', 'T85',\n",
    "        'RH85', 'U85', 'V85', 'HT85', 'T70', 'RH70', 'U70', 'V70',\n",
    "        'HT70', 'T50', 'RH50', 'U50', 'V50', 'HT50', 'KI', 'TT',\n",
    "        'SLP', 'SLP_', 'Precp', 'Ozone']\n",
    "    try:\n",
    "        with open(descr_path, 'r') as f:\n",
    "            descr = f.read()\n",
    "    except IOError:\n",
    "        print('Downloading file from', ozone_descr, file=sys.stderr)\n",
    "        r = requests.get(ozone_descr)\n",
    "        with open(descr_path, 'w') as f:\n",
    "            f.write(r.text)\n",
    "        descr = r.text\n",
    "        r.close()\n",
    "    try:\n",
    "        data = pd.read_csv(data_path, delimiter=',',\n",
    "                           na_values=['?'], names=columns, parse_dates=True)\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "    except IOError:\n",
    "        print('Downloading file from', ozone_data, file=sys.stderr)\n",
    "        r = requests.get(ozone_data)\n",
    "        with open(data_path, 'w') as f:\n",
    "            f.write(r.text)\n",
    "        r.close()\n",
    "        data = np.loadtxt(data_path, delimiter=',')\n",
    "    return Bunch(DESCR=descr,\n",
    "                 data=data.values[:, :72],\n",
    "                 feature_names=columns[:72],\n",
    "                 target=data.values[:, 72],\n",
    "                 target_names=['normal day', 'ozone day'])\n",
    "\n",
    "\n",
    "ozone = load_ozone()\n",
    "print(ozone.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a real dataset, we should treat it as a real problem.\n",
    "We take out a test set which we will not touch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    "    ozone.data, ozone.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train on the training set with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PCA(n_components=20),\n",
    "    SGDClassifier(loss='log', penalty='l1',\n",
    "                  max_iter=500, alpha=0.001, tol=0.01, class_weight={0: 1, 1: 25}))\n",
    "param_grid = {\n",
    "    'sgdclassifier__alpha': [0.001, 0.01, 0.1],\n",
    "    'sgdclassifier__tol': [0.001, 0.01, 0.1],\n",
    "}\n",
    "grid = GridSearchCV(model, param_grid, cv=5)\n",
    "grid.fit(xtrain, ytrain)\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the models during grid search may not converge.\n",
    "That's completely fine, we did root them out through the cross-validation.\n",
    "The best model is likely to be one that did converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "yfit = grid.best_estimator_.predict(xtest)\n",
    "print(classification_report(ytest, yfit, target_names=ozone.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey!  That wasn't online learning.\n",
    "\n",
    "Right, it was not.\n",
    "The ozone dataset is small enough to fit in memory\n",
    "and we are not expecting to receive any new data,\n",
    "yet we can make up that by slitting into two sets of data and train SDG in an online fashion.\n",
    "We will *misuse* the `train_test_split` function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ozone1, ozone2, yozone1, yozone2 = train_test_split(\n",
    "    ozone.data, ozone.target, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `sklearn` there are two ways of using online learning.\n",
    "One is to use a method called `partial_fit`, instead of `fit`,\n",
    "which will update parameters instead of fitting completely new ones.\n",
    "Another way to enable online learning is to pass `warm_start=True`,\n",
    "this forces `fit` to always work like `partial_fit`.\n",
    "\n",
    "Both methods only work on models that inherently support online learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain1, xtest1, ytrain1, ytest1 = train_test_split(\n",
    "    ozone1, yozone1, test_size=0.2, random_state=0)\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PCA(n_components=10),\n",
    "    SGDClassifier(loss='hinge', penalty='l1', max_iter=200, alpha=0.001,\n",
    "                  tol=0.001, warm_start=True, class_weight={0: 1, 1: 25}))\n",
    "model.fit(xtrain1, ytrain1)\n",
    "yfit = model.predict(xtest1)\n",
    "print(classification_report(ytest1, yfit, target_names=ozone.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know about half of the data and we can, more-or-less, classify that.\n",
    "But if we try to classify the data we do not know about we may run into trouble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain2, xtest2, ytrain2, ytest2 = train_test_split(\n",
    "    ozone2, yozone2, test_size=0.2, random_state=0)\n",
    "yfit = model.predict(xtest2)\n",
    "print(classification_report(ytest2, yfit, target_names=ozone.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train with some data from the second dataset and see if things improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xtrain2, ytrain2)\n",
    "yfit = model.predict(xtest2)\n",
    "print(classification_report(ytest2, yfit, target_names=ozone.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Non-GD Optimisation\n",
    "\n",
    "We saw SGD and said that it is the most often used optimization technique.\n",
    "But what are the others?\n",
    "One technique is **simulated annealing** which works by slow cooling.\n",
    "In summary: simulated annealing tries random neighbors at each iteration and keeps\n",
    "track of of the point with the lowest value of the cost function.\n",
    "The search space for a new neighbor (i.e. the maximum distance form the lowest point\n",
    "found until now) reduces at each iteration.\n",
    "This is similar to SGD with a decreasing learning rate.\n",
    "\n",
    "But there are more techniques.\n",
    "Notably **swarm intelligence** provides us with several optimization algorithms:\n",
    "\n",
    "- particle swarm\n",
    "- bat swarm\n",
    "- cuckoo search\n",
    "\n",
    "And **genetic algorithms** also work reasonably in an online learning scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[UCI - Ozone Level Detection Dataset][1]\n",
    "\n",
    "[1]: https://archive.ics.uci.edu/ml/datasets/ozone+level+detection \"UCI - Ozone\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
