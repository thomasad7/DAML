{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAML 10 - ANN Architectures\n",
    "\n",
    "Michal Grochmal <michal.grochmal@city.ac.uk>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from multiple perceptrons interconnected with each other,\n",
    "several neural network architectures have been developed over the years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Architectures\n",
    "\n",
    "- Deep Neural Networks are simply networks with lots of layers,\n",
    "  but training layers deep through backpropagation turns to be hard.\n",
    "  A *vanishing gradient* problem it tackled by careful selection of activation functions.\n",
    "\n",
    "- Convolutional Neural Nets are networks with one or more convolutional layers,\n",
    "  these layers are not fully connected providing feature selection based on parts\n",
    "  of the input.\n",
    "\n",
    "- Recurrent Neural Networks have connections going backwards, i.e. the output\n",
    "  of a neuron feeds into the input of a neuron in the same or previous layer.\n",
    "  RNN do not need to be deep networks but excel as such.\n",
    "\n",
    "- Long Short Term Memory (LSTM) are specifically constructed RNNs,\n",
    "  with very specific activation functions per layer.\n",
    "\n",
    "- Autoencoders are DNNs which can repeat the patterns they were presented with.\n",
    "  These are trained through unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Architectures\n",
    "\n",
    "- Hopfield Networks are early *autoencoders*, these could repeat a known\n",
    "  pattern when presented with a similar one.\n",
    "\n",
    "- Boltzman Machines are networks of probabilistic neurons where all neurons\n",
    "  are connected in all directions.\n",
    "  The input and output is done from the same neurons (visible neurons).\n",
    "\n",
    "- Restricted Boltzman Machines are BMs in which the hidden layer neurons\n",
    "  are not interconnected.\n",
    "  These are much easier to train than full BMs.\n",
    "\n",
    "- Deep Belief Networks are stacked RBMs on top of each other.\n",
    "  Each RBM can be trained separately, and we can stack several layers or RBMs.\n",
    "  These were the early DNNs.\n",
    "\n",
    "- Self-Organizing Maps are unsupervised networks for data visualization\n",
    "  and dimensionality reduction.\n",
    "  They use the concept that connections through which\n",
    "  data passes should be reinforced whilst all other connections should decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "\n",
    "The current top library for most neural network computing.\n",
    "It really is a directed acyclic graph (DAG) processing engine on top of tensors.\n",
    "Where tensors are pretty much matrices (e.g. NumPy arrays).\n",
    "Its main selling point is `tensorboard` a web UI to monitor the processing\n",
    "of the graph, and therefore monitor the network training.\n",
    "\n",
    "A demo of a similar (limited to a handful of 2-dimensional problems)\n",
    "interface can be found at [tensorflow playground][tfpl]\n",
    "\n",
    "[tfpl]: http://playground.tensorflow.org"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
