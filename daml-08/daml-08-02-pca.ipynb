{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAML 08 - Principal Component Analysis\n",
    "\n",
    "Michal Grochmal <michal.grochmal@city.ac.uk>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is an unsupervised algorithm which reduces the feature space.\n",
    "The algorithm builds a projection of the feature space into a space\n",
    "with a smaller number of dimensions.\n",
    "The new feature space is not necessarily meaningful in terms of the original features,\n",
    "because the projection plane (or hyperplane) may cross the features diagonally.\n",
    "\n",
    "The algorithm works by decomposing the space into eigenvectors and eigenvalues.\n",
    "Then it reorders the eigenvectors in order of how much variance they explain.\n",
    "Each eigenvector is parallel to each other,\n",
    "therefore reordering them does not impact the data representation.\n",
    "The maximum number of eigenvectors for data in $N$ dimensions is $N$\n",
    "but the power of PCA is that we can use less than $N$ eigenvectors\n",
    "to get a *\"good enough\"* representation of the data.\n",
    "Moreover, we know that these will be the first eigenvectors once the\n",
    "algorithm has ordered all eigenvectors in variance order.\n",
    "\n",
    "Let's build a small example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot two classes: red and blue, in three dimensions.\n",
    "The *meshgrid* produces all combinations between $x$ and $y$ features,\n",
    "and we build a almost-dependent $z$ feature.\n",
    "Since the $z$ feature is almost fully dependent on the values of $x$ and $y$,\n",
    "the variance in $z$ can be explained by the variance in $x$ and $y$.\n",
    "We expect that PCA will figure that out and flatten out the $z$ feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = 64\n",
    "x = np.linspace(0, 10, points) + 7*np.random.rand(points)\n",
    "y = np.linspace(0, 10, points) + 3*np.random.rand(points)\n",
    "gx, gy = np.meshgrid(x, y)\n",
    "z = 2*gx + gy + 15*np.random.rand(points, points)\n",
    "red = (z > 20) & (x < 10)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(gx[red], gy[red], z[red], c='crimson', alpha=0.7)\n",
    "ax.scatter(gx[~red], gy[~red], z[~red], c='steelblue', alpha=0.7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA expects to be given the number of components we project onto,\n",
    "this is a hyperparameter of the PCA algorithm.\n",
    "We are rather confident that we can take the data above and\n",
    "live with its variance in two dimensions only,\n",
    "therefore we will use 2 components.\n",
    "\n",
    "We use `fit_transform` on our model.\n",
    "This is equivalent to calling `fit` and `transform` on the same data.\n",
    "Where the `transform` is a typical method of unsupervised learning/preprocessing models,\n",
    "akin of the `predict` method of classification and regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "X = np.c_[gx.reshape(-1), gy.reshape(-1), z.reshape(-1)]\n",
    "pca = PCA(n_components=2)\n",
    "X_new = pca.fit_transform(X)\n",
    "red_new = red.reshape(-1)\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.scatter(X_new[red_new, 0], X_new[red_new, 1], color='crimson', alpha=0.7)\n",
    "ax.scatter(X_new[~red_new, 0], X_new[~red_new, 1], color='steelblue', alpha=0.7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That look good but how PCA makes this projection?\n",
    "The main part of the PCA algorithm is singular value decomposition.\n",
    "Assuming that our data is centered at zero\n",
    "(something easy to perform, just subtract the mean in every dimension)\n",
    "we can decompose the samples matrix $M$ as:\n",
    "\n",
    "$$M = U \\Sigma V^* $$\n",
    "\n",
    "Both $U$ and $V$ are unitary matrices, and $\\Sigma$ is a diagonal matrix.\n",
    "Since we have only real numbers in the matrices we can shift values around and end with:\n",
    "\n",
    "$$M = Q \\Sigma Q^* $$\n",
    "\n",
    "i.e. we made $U$ and $V$ to be unitary to each other.\n",
    "Now $Q$ contains the eigenvectors - components - of the original matrix\n",
    "and $\\Sigma$ contains the eigenvalues - explained variance.\n",
    "We can swap numbers in the diagonal of $\\Sigma$\n",
    "as long as we swap the same rows in $Q$,\n",
    "therefore we can order the vectors by the magnitude of the value\n",
    "in the diagonal of $\\Sigma$.\n",
    "\n",
    "PCA keeps track of the eigenvectors/components and eigenvalues/variance.\n",
    "This allows us to plot how the projection has been performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(gx[red], gy[red], z[red], c='crimson', alpha=0.2)\n",
    "ax.scatter(gx[~red], gy[~red], z[~red], c='steelblue', alpha=0.2);\n",
    "\n",
    "M = np.c_[gx.mean(), gy.mean(), z.mean()]\n",
    "sizes = pca.explained_variance_ratio_ / pca.explained_variance_ratio_.min()\n",
    "eigen = pca.components_ * sizes[:, np.newaxis]\n",
    "ax.quiver(M[[0, 0], 0], M[[0, 0], 1], M[[0, 0], 2],\n",
    "          eigen[:, 0], eigen[:, 1], eigen[:, 2],\n",
    "          linewidths=3, color='seagreen', length=3, arrow_length_ratio=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenfaces\n",
    "\n",
    "When used on faces,\n",
    "and since eigenvectors have been used on faces over the years in many applications,\n",
    "the eigenvectors are often called *eigenfaces*.\n",
    "The eigenfaces represent the biggest directions of variations across a dataset of faces,\n",
    "and are one of the most common techniques in face recognition.\n",
    "\n",
    "To get some faces we will use the *Labeled Faces in the Wild* dataset,\n",
    "which contains thousands of cropped photos to show only the face.\n",
    "This dataset has been produced in early 2000s, therefore has mostly people often photographed at that time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=30)\n",
    "print(faces.target_names)\n",
    "faces.images.shape, faces.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every image is 62 times 47 pixels, or 2914 pixels in total.\n",
    "We argue that we have 2914 different features.\n",
    "`matplotlib` provides us with `imshow`.\n",
    "Since we are working with images, this is perfect to see what we are working with.\n",
    "Note that the orientation of the faces changes between the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 8, figsize=(16, 5), subplot_kw={'xticks':[], 'yticks':[]})\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(faces.data[i].reshape(62, 47), cmap='binary_r')\n",
    "    ax.set_title(faces.target_names[faces.target[i]], fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rule of thumb is that, for images, $2 \\sqrt N$ is a good number of reduced features.\n",
    "We will use 128, since it is close to the square root of 2914 and a power of 2.\n",
    "PCA `components_` are the eigenvector orientation, i.e. the direction of the vector\n",
    "in the dimensions of the original dataset.  The `explained_variance_` are the vector\n",
    "magnitudes.\n",
    "\n",
    "We will use a *randomized* solver for this dataset.\n",
    "The randomized solver uses a heuristic method to identify the first couple of eigenvectors.\n",
    "Solving the decomposition problem analytically is actually very costly for big datasets,\n",
    "the randomized solver alleviates this considerably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=128, svd_solver='randomized')\n",
    "pca.fit(faces.data)\n",
    "pca.components_, pca.components_.shape, pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we can see how the eigenvectors actually look.\n",
    "We can display images of 2914 features/pixels,\n",
    "the eigenvectors have exactly the same number of dimensions so we can plot them the same way.\n",
    "These are the actual *eigenfaces*.\n",
    "\n",
    "We plot the eigenfaces in order of explained variance,\n",
    "from the eigenface that explains the most in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6, 7, figsize=(14, 12), subplot_kw={'xticks':[], 'yticks':[]})\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(pca.components_[i].reshape(62, 47), cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important eigenfaces/eigenvectors are changes\n",
    "in illumination and orientation of the face.\n",
    "At first sight we may argue that that would be a problem if we would try to build a classifier.\n",
    "By the contrary, the PCA managed to move (most of) the illumination and orientation\n",
    "into their own dimensions, which means they were removed from the other dimensions.\n",
    "We could tweak the PCA output to prevent the first eigenvectors from being used.\n",
    "Yet, that's a topic for another time.\n",
    "\n",
    "We saw that we order the eigenvectors in explained variance order.\n",
    "But how much variance each eigenvector explains?\n",
    "We often want to plot a cumulative explained variance,\n",
    "to understand how many components we actually need.\n",
    "Let's rebuild the PCA with 512 components\n",
    "(we have no need to build all 2914 eigenvectors)\n",
    "and evaluate whether our choice of 128 components was good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=512, svd_solver='randomized')\n",
    "pca.fit(faces.data)\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "ax.set(xlabel='components', ylabel='explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transform` of the PCA technique projects the data into the space defined by the eigenvectors.\n",
    "This operation **loses some information** but since the majority\n",
    "of the variance is retained the loss is as minimal as possible.\n",
    "The `inverse_transform` can throw the projection back into the original dimensions,\n",
    "yet the data loss remains since we cannot guess that data.\n",
    "\n",
    "This allows us to visualize how much data was actually lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = {}\n",
    "for i in [128, 256]:\n",
    "    pca = PCA(n_components=i, svd_solver='randomized')\n",
    "    pca.fit(faces.data)\n",
    "    reduced = pca.transform(faces.data)\n",
    "    proj[i] = pca.inverse_transform(reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have projected the images into 128 and 256 dimensions.\n",
    "And if we plot them next to each other we can visually inspect the data loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 16, figsize=(16, 5), subplot_kw={'xticks':[], 'yticks':[]})\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "for i in range(16):\n",
    "    ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap='binary_r')\n",
    "    ax[1, i].imshow(proj[128][i].reshape(62, 47), cmap='binary_r')\n",
    "    ax[2, i].imshow(proj[256][i].reshape(62, 47), cmap='binary_r')\n",
    "ax[0, 0].set_ylabel('full')\n",
    "ax[1, 0].set_ylabel('128-dim');\n",
    "ax[2, 0].set_ylabel('256-dim');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digits\n",
    "\n",
    "Another dataset of images are the MNIST handwritten digits.\n",
    "This dataset is often used for algorithm benchmark due to its complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits['DESCR'])\n",
    "digits.data.shape, digits.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of dimensions (64) in this dataset is much smaller\n",
    "than in the faces dataset.\n",
    "Moreover, we know very well the labels: digits from 0 to 9.\n",
    "We may as well try to visualize the distribution of each digit.\n",
    "In other words, we'll try to reduce the digits to a representation in two dimensions.\n",
    "\n",
    "These are 8 pixels by 8 pixels images.\n",
    "As we did with the faces we can plot them to see what we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = digits.images[:20, :]\n",
    "fig, ax = plt.subplots(4, 5, figsize=(16, 8))\n",
    "for i in range(20):\n",
    "    ax.flat[i].imshow(sample[i], cmap='binary')\n",
    "    ax.flat[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the analytic solver for a small number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "proj = pca.fit_transform(digits.data)\n",
    "digits.data.shape, proj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the digits as a representation in two dimensions.\n",
    "On important thing of `fit_transform` (and `transform` too)\n",
    "is that it keeps the order of the samples it is fed.\n",
    "This is true for almost all operation in `sklearn`.\n",
    "\n",
    "Since we have the same order of the samples in two dimensions,\n",
    "and the dataset provides us with the correct labels for the digits,\n",
    "we can plot each digit with a specific color.\n",
    "We build the colors from a color map and even add a label\n",
    "in the median of the digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patheffects as mplpf\n",
    "fig, ax = plt.subplots(figsize=(16, 14))\n",
    "plot = ax.scatter(proj[:, 0], proj[:, 1], s=30, c=digits.target,\n",
    "                  alpha=0.7, cmap=plt.cm.get_cmap('viridis', 10))\n",
    "plot.set_clim(-0.5, 9.5)  # make the ticks look right\n",
    "ax.axis('off')\n",
    "ticks = np.array(range(10))\n",
    "fig.colorbar(plot, ax=ax, aspect=50, ticks=ticks)\n",
    "for i in range(10):\n",
    "    xtext, ytext = np.median(proj[digits.target == i, :], axis=0)\n",
    "    txt = ax.text(xtext, ytext, str(i), fontsize=24)\n",
    "    txt.set_path_effects([\n",
    "        mplpf.Stroke(linewidth=6, foreground='white'), mplpf.Normal()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That may look colorful but isn't very promising.\n",
    "We can see the 5 and 8 or 1 and 7 are very related to each other,\n",
    "which is what we expect since we know how digits look.\n",
    "Yet, the separation between classes is very poor.\n",
    "\n",
    "Let's build the full PCA (on all dimensions) and see the cumulative explained variance.\n",
    "This should give us an idea of where we went wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(digits.data)\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "ax.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "ax.set(xlabel='components', ylabel='explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 2 components do a very bad job of explaining the dataset,\n",
    "even 10 components would do a bad job.\n",
    "PCA will not allow us to visualize this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearity of PCA and manifold techniques\n",
    "\n",
    "The issue with the digits happens because PCA is a technique\n",
    "that depends on the linearity of the dimensions it works with.\n",
    "Relations that are non-linear,\n",
    "e.g. two open spaces means an 8, one open space means a 6 or a 9,\n",
    "no open spaces within the digit means anything else;\n",
    "cannot be captured by PCA.\n",
    "\n",
    "To deal with non-linear relationships manifold techniques were developed.\n",
    "These techniques are based on maintaining distances between points in the data\n",
    "instead of maintaining the variance within specific dimensions.\n",
    "That said, PCA is easy to interpret due to the cumulative variance,\n",
    "yet it is often difficult to interpret what a manifold technique is doing.\n",
    "Moreover, there is no definitive measure of whether a manifold technique will\n",
    "or will not converge for a particular dataset,\n",
    "and manifold techniques are more sensitive to outliers than PCA.\n",
    "\n",
    "As a rule of thumb,\n",
    "it is wise to attempt manifold techniques only after attempting PCA\n",
    "and understanding the shape of the data.\n",
    "Some manifold techniques include:\n",
    "\n",
    "-   Multidimensional Scaling (MDS) is the simplest manifold which works by preserving\n",
    "    (as much as possible) the distances between *all* points in the dataset.\n",
    "\n",
    "-   Locally Linear Embedding (LLE) works like MDS but only preserves distances within\n",
    "    a defined number of neighbors, this allows to \"unroll\" certain relationships.\n",
    "\n",
    "-   Isomap is similar to LLE in that it uses a neighbor search but then computes\n",
    "    eigenvectors over the local groups.\n",
    "\n",
    "-   Spectral Embedding is a stochastic way of building eigenmaps from a graph based\n",
    "    neighbor search, this makes it similar to LLE.\n",
    "\n",
    "-   t-distributed Stochastic Neighbor Embedding (t-SNE) may reveal structures at\n",
    "    different scales since it builds several t-distributions (similar to a bell shape)\n",
    "    within groups of neighbors and then works with these t-distributions instead\n",
    "    of the actual data points."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
