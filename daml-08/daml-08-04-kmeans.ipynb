{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAML 08 - kmeans\n",
    "\n",
    "Michal Grochmal <michal.grochmal@city.ac.uk>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **expectation-maximization** (E-M) algorithm is often useful in machine learning.\n",
    "It consists of the repetition of E-steps (expectation) and M-steps (maximization) until\n",
    "the algorithm converges to a point where more steps would not change the solution.\n",
    "In other words at each step some **fitness function** comes closer to an optimal value.\n",
    "In general the algorithm looks as follows:\n",
    "\n",
    "1.  Assign data to the expected solution from previous step (E-step)\n",
    "2.  Change the solution so that the current state of data maximizes a fitness function (M-step)\n",
    "3.  Repeat until convergence\n",
    "\n",
    "The `kmeans` algorithm is a clear implementation of expectation-maximization,\n",
    "the algorithm works as follows:\n",
    "\n",
    "1.  Assign random cluster centers\n",
    "2.  (E-step) assign data points, according to some distance measure, to the closest cluster center\n",
    "3.  (M-step) create new cluster centers from the mean of currently assigned data points\n",
    "\n",
    "Note that the above requires some form of **distance measure**, and that the *fitness function*\n",
    "is the minimization of all the distances.\n",
    "Let's do the initial imports and dive into `kmeans`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `sklearn`'s samples generator `make_blobs` to build three clusters of points.\n",
    "the blobs will be between -10 and 10, and will be preassigned to classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "cluster_num = 3\n",
    "X, y = make_blobs(n_samples=128, centers=cluster_num, cluster_std=0.5)\n",
    "fig, ax = plt.subplots(figsize=(14, 9))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement an *expectation-maximization* algorithm ourselves.\n",
    "The following is a very simple implementation of `kmeans`.\n",
    "We perform the expectation step by first randomly assigning cluster centers,\n",
    "later the expectation step is performed by taking the mean of the points in the cluster.\n",
    "The maximization step is performed using `pairwise_distances` to the currently\n",
    "assigned cluster centers.\n",
    "\n",
    "We plot the results.\n",
    "Since `kmeans` has the tendency of choking when a cluster center is not close enough to at least\n",
    "one point, we may need to run the following several times until a good solution is found.\n",
    "Note that we use a heuristic about initial cluster location,\n",
    "we assume that our data is centers on zero and assign cluster centers\n",
    "within the mean variance of the data from the origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "steps = 6\n",
    "clusters = np.random.rand(3, 2)*X.mean()\n",
    "colors = np.array(['crimson', 'steelblue', 'seagreen'])\n",
    "fig, ax = plt.subplots(3, 2, figsize=(16, 9))\n",
    "for i in range(steps):\n",
    "    distances = pairwise_distances(X, clusters)\n",
    "    classes = distances.argmin(axis=1)\n",
    "    ax.flat[i].scatter(clusters[:, 0], clusters[:, 1], c=colors);\n",
    "    ax.flat[i].scatter(X[:, 0], X[:, 1], c=colors[classes], alpha=0.3);\n",
    "    ax.flat[i].set(xticks=[], yticks=[], title=f'step {i}')\n",
    "    new_clusters = np.array([]).reshape(0, 2)\n",
    "    for j in range(cluster_num):\n",
    "        centre = X[classes == j]\n",
    "        if centre.shape[0]:\n",
    "            new_clusters = np.r_['0,2', new_clusters, centre.mean(axis=0)]\n",
    "        else:\n",
    "            new_clusters = np.r_['0,2', new_clusters, clusters[j, :]]\n",
    "        ax.flat[i].annotate('', new_clusters[j, :], clusters[j, :],\n",
    "                            arrowprops=dict(arrowstyle='->', linewidth=1))\n",
    "    clusters = new_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "One limitation of `kmeans` that's apparent on first sight is that we need to know\n",
    "a-priori the number of clusters we want to assign the data to.\n",
    "Although **cluster inertia** measures across several amounts of clusters or a handful\n",
    "of statistical techniques  (e.g. silhouette analysis) exist to estimate a good number of clusters,\n",
    "these are far from perfect.\n",
    "Preexisting knowledge of the data,\n",
    "possibly aided by dimensionality reduction is often needed to find a good number of clusters.\n",
    "\n",
    "The expectation-maximization algorithm requires stochastic initialization and,\n",
    "given a bad initial start, may find a local maximum instead of a global maximum.\n",
    "In other words badly initialized `kmeans` can cluster badly.\n",
    "In practice E-M algorithms shall run for several random initializations and then be evaluated.\n",
    "Below the hood this is what the `sklearn` version of `kmeans` does,\n",
    "it runs the model fit several times and selects the best model.\n",
    "(The number of times the `kmeans` algorithm is run by `sklearn` is defined\n",
    "by the `n_init` argument to the class initialization.)\n",
    "\n",
    "`kmeans` is a linear algorithm and will only find clusters defined by linear borders.\n",
    "Graph techniques such as spectral clustering or some versions of hierarchical clustering\n",
    "are capable of dealing directly with non-linear clusters.\n",
    "Yet, another viable trick is to preprocess the data with a manifold technique\n",
    "and then cluster the linearized data with (possibly a variant of) `kmeans`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwritten Digits\n",
    "\n",
    "Without knowing of any labels `kmeans` can deal with quite complex problems.\n",
    "We saw that the digits dataset is not particularly easy since it contains\n",
    "non-linear relationships.\n",
    "Let's see what `kmeans` can take out of that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time around we make our life easier and use `sklearn`'s implementation of `kmeans`.\n",
    "One of the hyperparameters is the number of clusters, which for the digits is easy to know.\n",
    "Since we work with digits we know that we are after 10 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "clusters = kmeans.fit_predict(digits.data)\n",
    "kmeans.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used `fit_predict`, which is similar to `fit_transform`.\n",
    "We have build the model (the `fit` part) and predicted the classes based\n",
    "on the cluster centers built by this model.\n",
    "Contrary to supervised learning,\n",
    "it is common to predict on the same data in unsupervised learning.\n",
    "This is because this way we can evaluate the model against other measures on the same data.\n",
    "\n",
    "The cluster centers are shaped just like the images we worked with.\n",
    "Since we are clustering in the same number of dimensions as the images,\n",
    "Awe can build images from the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(16, 6), subplot_kw={'xticks':[], 'yticks':[]})\n",
    "centers = kmeans.cluster_centers_.reshape(10, 8, 8)\n",
    "for ax, center in zip(axes.flat, centers):\n",
    "    ax.imshow(center, cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can make up some digits out of this.\n",
    "It looks promising.\n",
    "That said `kmeans` was never told what labels these digits,\n",
    "it can only know that it should look for 10 different digits in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.shape, np.unique(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume that the algorithm did most of its job right perhaps we can label these clusters.\n",
    "In other words, we actually know the labels of these digits,\n",
    "if we figure out the label that most often appear in a cluster we label the cluster with that label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "label_map = {}\n",
    "for i in range(10):\n",
    "    label_map[i] = mode(digits.target[clusters == i])[0]\n",
    "label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly right.\n",
    "Now we can build an array of what `kmeans` placed in what bag,\n",
    "this will spit out something that can be scored in a similar way to a supervised algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.zeros_like(clusters)\n",
    "for k, v in label_map.items():\n",
    "    labels[clusters == k] = v\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default `kmeans` scoring is by **cluster inertia**,\n",
    "the sum of all distances form points to its cluster center.\n",
    "This allows to compare two `kmeans` runs against each other.\n",
    "Notably, `sklearn` performs `kmeans` several times and then compares the inertia.\n",
    "But this is not a good measure to compare `kmeans` between datasets or against other models.\n",
    "\n",
    "Evaluating unsupervised algorithms between themselves is always a tricky area.\n",
    "Here we knew the labels, therefore we can use supervised learning scoring,\n",
    "e.g. F1 or weighted accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "accuracy_score(digits.target, labels), f1_score(digits.target, labels, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an algorithm that only knew that there are 10 digits in the data but had\n",
    "no idea how they look like that is a pretty amazing score.\n",
    "Let's look at what got misclassified the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "mat = confusion_matrix(digits.target, labels)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = sns.heatmap(mat.T, square=True, annot=True, fmt='d', cmap='viridis',\n",
    "                 xticklabels=digits.target_names, yticklabels=digits.target_names)\n",
    "ax.set(xlabel='true label', ylabel='predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Distance Measures\n",
    "\n",
    "We have only dealt with *euclidean distances* but that is not the only distance measure available.\n",
    "In machine learning algorithm and in literature you will often find mention of *L-norms*.\n",
    "An *L-norm* (also called $L^p$ space) often written as L1, L2, $\\|\\cdot\\|_1$ or $\\|\\cdot\\|_2$\n",
    "is a notation of a specific measure of distance.\n",
    "The most important one being L2 ($\\|\\cdot\\|_2$), the euclidean distance.\n",
    "The L-norm is defined, for a vector of $i$ components, as:\n",
    "\n",
    "$$\\|x\\|_k = \\left( \\sum_{i=0}^{N} \\lvert x_i \\rvert ^k \\right)^{\\frac{1}{k}}$$\n",
    "\n",
    "Therefore $\\|x\\|_2$ (or L2) is our well known\n",
    "\n",
    "$$\\|x\\|_2 = \\sqrt{\\sum_{i=0}^{N} x_i^2}$$\n",
    "\n",
    "The sum of absolutes, or $\\|x\\|_1$ (or L1) turns to be resistant against outliers,\n",
    "and is sometimes preferred as a distance measure\n",
    "\n",
    "$$\\|x\\|_1 = \\sum_{i=0}^{N} \\lvert x_i \\rvert$$\n",
    "\n",
    "L1 is also often called Manhattan distance due to the square shaped blocks in America."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
