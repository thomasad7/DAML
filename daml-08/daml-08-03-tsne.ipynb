{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAML 08 - t-SNE\n",
    "\n",
    "Michal Grochmal <michal.grochmal@city.ac.uk>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-distributed Stochastic Neighbor Embedding is a manifold technique created\n",
    "by Geoffrey Hinton and Laurens van der Maaten.\n",
    "It uses a probabilistic distribution to measure distances between points in a dataset\n",
    "(high probability means short distance and low probability long distance).\n",
    "It then attempts to keep these distances during transformation.\n",
    "\n",
    "t-SNE is considerably different from a decomposition technique such as PCA:\n",
    "\n",
    "-   Has no inverse transform (this is often the case with manifold techniques)\n",
    "\n",
    "-   The number of components is always much smaller than the number of dimensions\n",
    "    in the original data.  Since this originally is a visualization technique,\n",
    "    most often you will only see 2 or 3 components\n",
    "\n",
    "-   Since it is a stochastic technique the result is considerably different\n",
    "    depending on the initial (random) state, or even the ordering of the input data\n",
    "\n",
    "We import the usual stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how t-SNE performs on the MNIST handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "sample = digits.images[:20]\n",
    "fig, axes = plt.subplots(4, 5, figsize=(16, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(sample[i], cmap='binary')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-SNE algorithm in `sklearn` works in a similar way as any other preprocessor,\n",
    "We give it the hyperparameters - here just the number of components - and `fit` the model.\n",
    "We use `fit_transform` to already transform the data.\n",
    "\n",
    "The following takes a while to run, t-SNE is computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "proj = TSNE(n_components=2).fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did with PCA we plot the groups output by t-SNE,\n",
    "and we take the known labels and color the groups accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patheffects as mplpf\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(proj[:, 0], proj[:, 1], c=digits.target, cmap='viridis')\n",
    "ax.axis('off')\n",
    "for i in range(10):\n",
    "    xtext, ytext = np.median(proj[digits.target == i, :], axis=0)\n",
    "    txt = ax.text(xtext, ytext, str(i), fontsize=24)\n",
    "    txt.set_path_effects([\n",
    "        mplpf.Stroke(linewidth=6, foreground='white'), mplpf.Normal()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The separation between classes is pretty good.\n",
    "t-SNE can find non-linear relations therefore it performs\n",
    "much better than PCA on a non-linear dataset.\n",
    "That said, t-SNE is not without its own flaws.\n",
    "For example, the algorithm is very sensitive to data ordering,\n",
    "and we can test it out by reordering our data.\n",
    "\n",
    "Digits data is ordered as several groups from 0 to 9.\n",
    "We will change that ordering: first all zeros, then all ones, and so on;\n",
    "and run t-SNE with that ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([digits.data[digits.target==i] for i in range(10)])\n",
    "y = np.hstack([digits.target[digits.target==i] for i in range(10)])\n",
    "proj_ordered = TSNE(n_components=2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way we plotted it before we do the ordered digits projection.\n",
    "We can then compare both plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(proj_ordered[:, 0], proj_ordered[:, 1], c=y, cmap='viridis')\n",
    "ax.axis('off')\n",
    "for i in range(10):\n",
    "    xtext, ytext = np.median(proj_ordered[y == i, :], axis=0)\n",
    "    txt = ax.text(xtext, ytext, str(i), fontsize=24)\n",
    "    txt.set_path_effects([\n",
    "        mplpf.Stroke(linewidth=6, foreground='white'), mplpf.Normal()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class separation is still good but the plot looks very different.\n",
    "That is the big disadvantage of t-SNE, the algorithm is not trivially reproducible.\n",
    "That said, t-SNE will certainly be a better preprocessing technique than PCA for thsi dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
