{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAML 05 - Stats\n",
    "\n",
    "Michal Grochmal <michal.grochmal@city.ac.uk>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To not make mistakes in analysis a deal of statistical knowledge is required.\n",
    "We will review some statistics and learn a little about distributions in `scipy`.\n",
    "`scipy` is the mathematical library for Python on top of NumPy.\n",
    "It was geared to be the one and only mathematic library for the sciences in Python\n",
    "but it turned out that it would become too big.\n",
    "Libraries flensed from `scipy` often include the `sci` at the beginning of its name,\n",
    "e.g. `scikit-learn` or `scikit-image`.\n",
    "\n",
    "`scipy` is comprised of:\n",
    "\n",
    "- Numerical Integration\n",
    "- Function Optimization - used in machine learning routines\n",
    "- Interpolation - e.g. splines\n",
    "- Fast Fourier Transforms\n",
    "- General Signal Processing\n",
    "- Linear Algebra - including Matrix decomposition\n",
    "- Image Processing - as NumPy arrays, used by `scikit-image`\n",
    "- Sparse Matrices - and graphs\n",
    "- Statistics - which is what interests us right now\n",
    "- And a handful of extra things\n",
    "\n",
    "Several of these routines are used in `scikit-learn` and `scikit-image`\n",
    "to produce decomposition and machine learning algorithms.\n",
    "We will look at machine learning from a higher perspective but,\n",
    "for the statistics we need, we can use the `scipy.stats` module.\n",
    "\n",
    "As a quick review let's see a handful of statistical measures we can perform on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean\n",
    "\n",
    "$$\\bar{x} = \\frac{1}{N} \\sum_{i=1}^{N} x_i$$\n",
    "\n",
    "#### Variance\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\bar{x})^2$$\n",
    "\n",
    "#### Standard Deviation\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\bar{x})^2}$$\n",
    "\n",
    "#### Covariance\n",
    "\n",
    "$$cov(X, Y) = \\sigma_{xy} = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y})$$\n",
    "\n",
    "#### Correlation\n",
    "\n",
    "$$corr(X, Y) = r = \\frac{cov(X, Y)}{\\sigma_x \\sigma_y} = \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_y}$$\n",
    "\n",
    "Note: $1/N$ often becomes $1/(N-1)$ in bias-corrected calculations.\n",
    "Bias correction is needed when operating over a sample instead of operating over\n",
    "the entire population.  All below `NumPy` functions (except correlation functions\n",
    "which is not multiplied by $1/N$) accept a `ddof=` (degrees of freedom)\n",
    "argument to perform a sample based calculation.\n",
    "\n",
    "First let's create some arrays to play with.\n",
    "One array is a simple range and the others are noise perturbed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.arange(30)\n",
    "acv = np.arange(30) + np.random.rand(30) - 1\n",
    "acr = np.arange(30) + np.random.rand(30) - 1\n",
    "arr, acv, acr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NumPy` has the mean method but implementing it by hand is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arr.mean())\n",
    "print(arr.sum() / len(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation with zero degrees of freedom is the deviation of our data.\n",
    "With one degree of freedom it is an estimate of a population from which\n",
    "whe data at hand may be a reasonable sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arr.std())\n",
    "print(np.std(arr, ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same story with the variance (since it is just the squared standard deviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arr.var())\n",
    "print(arr.var(ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance method (`cov`) produces the variance of each array on the diagonal\n",
    "and the actual covariance on the interception between the arrays.  i.e.\n",
    "\n",
    "| `np.cov` | arr            | acv           |\n",
    "|:-------- |:-------------- |:------------- |\n",
    "| **arr**  | cov(arr, arr)  | cov(arr, acv) |\n",
    "| **acv**  | cov(acv, arr)  | cov(acv, acv) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.cov([arr, acv], ddof=0))\n",
    "print(np.cov([arr, acv], ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation measure we saw above is actually just one way of measuring correlation.\n",
    "It is called the Person's correlation coefficient or just Person's r.\n",
    "`NumPy`'s `corrcoef` produces a Matrix similar to the covariance method.\n",
    "\n",
    "One important thing to note is that Pearson's correlation coefficient\n",
    "divides out the degrees of freedom in its equation.\n",
    "Therefore, if we are working with a sample, we cannot bias-correct it.\n",
    "`scipy.stats` provides us with a `pearsonr` method,\n",
    "which attempts to estimate whether an uncorrelated population could give us the sample\n",
    "for which we got the correlation coefficient.\n",
    "In other words, it performs a rough *null hypothesis test* of attempting to generate\n",
    "a population which is not correlated but would result in the same coefficient.\n",
    "The second return from `scipy.stats.pearsonr` is the *p-value* of that tests.\n",
    "The test is rather rough, therefore it only makes sense for bug amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.corrcoef([arr, acv, acr]))\n",
    "print(stats.pearsonr(arr, acv))\n",
    "print(stats.pearsonr(acv, acr))\n",
    "print(stats.pearsonr(arr, acr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions\n",
    "\n",
    "We saw all these measures about the data, at the bottom of it we have the standard deviation.\n",
    "Why is this standard deviation thing important?\n",
    "Because it describes how the data variates from the mean,\n",
    "and how that moves from the mean becomes apparent if we assume that our data\n",
    "can be described by a Gaussian distribution.\n",
    "\n",
    "Let's plot one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, mean=0, std=1, var=1):\n",
    "    return np.exp(-np.square(x-mean)/2*var)/(np.sqrt(2*np.pi*var))\n",
    "\n",
    "arrow = {'facecolor': 'black', 'arrowstyle': '->'}\n",
    "text = {'fontsize': 14, 'ha': 'center', 'arrowprops': arrow}\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "x = np.linspace(-4, 4, 128)\n",
    "y = gaussian(x)\n",
    "ax.plot(x, y, color='navy')\n",
    "x = np.linspace(-2, 2, 128)\n",
    "y = gaussian(x)\n",
    "ax.fill_between(x, y, alpha=0.2, color='seagreen')\n",
    "ax.annotate('94.2%', xy=(-1.5, 0.05), xytext=(-3, 0.2), **text)\n",
    "\n",
    "x = np.linspace(-1, 1, 128)\n",
    "y = gaussian(x)\n",
    "ax.fill_between(x, y, alpha=0.5, color='crimson')\n",
    "ax.annotate('68.2%', xy=(0.5, 0.2), xytext=(1.5, 0.35), **text)\n",
    "\n",
    "x = np.array([-1, 1])\n",
    "y = gaussian(x)\n",
    "ax.scatter(x, y, s=100, color='navy')\n",
    "ax.annotate('inflection point', xy=(x[1]+0.07, y[1]+0.001), xytext=(3, 0.27), **text)\n",
    "\n",
    "def plot_std(ax, xy_left, xy_right):\n",
    "    arrow['arrowstyle'] = '<->'\n",
    "    ax.annotate('', xy=xy_left, xytext=xy_right, **text)\n",
    "    xcentre = xy_left[0] + (xy_right[0] - xy_left[0])/2\n",
    "    ax.text(xcentre, xy_left[1] + 0.01, 'std', ha='center', fontsize=14)\n",
    "    arrow['arrowstyle'] = '->'\n",
    "\n",
    "plot_std(ax, (-2, 0.01), (-1, 0.01))\n",
    "plot_std(ax, (-1, 0.01), (0, 0.01))\n",
    "plot_std(ax, (0, 0.01), (1, 0.01))\n",
    "plot_std(ax, (1, 0.01), (2, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The areas below the curve and the inflection points vary according how the Gaussian changes.\n",
    "But the relations always stay true:\n",
    "\n",
    "- The standard deviation is the vertical distance between the mean and the inflection point.\n",
    "- The area within two standard deviations around the mean contains the majority of data.\n",
    "- The area within four standard deviations contains roughly 95% of the data.\n",
    "\n",
    "But all that assumes that our data is modeled by a Gaussian distribution.\n",
    "Bear with me for a moment on this.\n",
    "\n",
    "The Gaussian distribution above is a continuous distribution but there are as well\n",
    "discrete distributions.  The discrete equivalent to the Gaussian distribution\n",
    "is the Binomial distribution, which is pretty much the histogram of a Gaussian.\n",
    "\n",
    "Yet, other distribution exist, for example the Poisson discrete distribution\n",
    "models occurrence of an event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "x = np.arange(64)\n",
    "for mu in [1, 5, 10, 30, 50]:\n",
    "    y = stats.poisson.pmf(x, mu)\n",
    "    label = '$\\mu = %s$' % mu\n",
    "    ax.plot(x, y, label=label)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the continuous version of the event based distribution is the Weibull distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull(x, gamma, k):\n",
    "    return (k/gamma) * (x/gamma)**(k-1) * np.exp(-(x/gamma)**k)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "x = np.linspace(0, 4, 256)\n",
    "for k in [1, 1.5, 3, 5, 10]:\n",
    "    y = weibull(x, 1, k)\n",
    "    label = 'k = %s' % k\n",
    "    ax.plot(x, y, label=label)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the data can be modeled by quite a lot of different distributions, right?\n",
    "Yes.  Yet in the vast majority of cases a Gaussian distribution turns out to be the best model.\n",
    "This is because the sum of several continuous distributions can be approximated as a Gaussian distribution.\n",
    "In other words, when distinct processes influence the data we measure, it is most likely\n",
    "that we will see a Gaussian distribution.\n",
    "Since each process affects the measures in a way that may result in some distribution,\n",
    "a group of distinct processes will produce a sum of distinct distributions,\n",
    "which tend towards a Gaussian.\n",
    "\n",
    "That works for continuous distributions, the sum of discrete distributions vary,\n",
    "yet *often* can be approximated with the Uniform distribution.\n",
    "\n",
    "Let's try this out by summing together several Weibull distributions.\n",
    "We plot here the evolution (cumulative sum) of adding more and more Weibull distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 4, 256)\n",
    "y = []\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "for k in np.linspace(1, 100, 10):\n",
    "    y.append(weibull(x, 1, k))\n",
    "s = np.cumsum(np.array(y), axis=0)\n",
    "k = 1\n",
    "for i in s:\n",
    "    label = '$k = %s$' % str(k)\n",
    "    k += 11  # same as the linspace call above\n",
    "    ax.plot(x, i, label=label)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "That last line definitely looks like a Gaussian.\n",
    "Most of the time we will be working with data that is generated by a complex process,\n",
    "i.e. by a combination of several processes.\n",
    "Therefore the Gaussian assumption and the use of standard deviation is justified.\n",
    "(Of course, if things do not work looking for another distribution is an option.)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
