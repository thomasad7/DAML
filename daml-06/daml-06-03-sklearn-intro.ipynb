{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAML 06 - sklearn Introduction\n",
    "\n",
    "Michal Grochmal <michal.grochmal@city.ac.uk>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sklearn` module in Python tries to *standardize the way machine learning\n",
    "techniques are performed*.  Instead of technique specific setup and manual application\n",
    "of the resulting model, `sklearn` proposes a standard class-based API.\n",
    "Every ML technique in `sklean` uses the following steps to produce a single model.\n",
    "\n",
    "1.  Every ML technique (model type) is a class that you can import\n",
    "2.  You feed the *hyperparameters* to the constructor, a new instance of a model\n",
    "    is bound to the hyperparameters passed in\n",
    "3.  You call `.fit()` on the input data, which has a standardized form, see below\n",
    "4.  For models that predict data you use `.predict()` to appl the model\n",
    "5.  For models that transform the data you use `.transform()`, some models have\n",
    "    a `.fit_transform()` which will fit and transform the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and Output\n",
    "\n",
    "Injecting data and reading the outputs of a model can often be troublesome,\n",
    "figuring out in which format a model wants its parameters is no easy task.\n",
    "Developers of `sklearn` tried to sort the misery of different libraries\n",
    "requiring parameters in different formats, and created a standard way in which\n",
    "all `sklearn` models receive the input.\n",
    "\n",
    "Ignore the code below, this is just to draw the picture below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "ax.axis('off')\n",
    "ax.axis('equal')\n",
    "ax.vlines(range(4), ymin=0, ymax=6, lw=1)\n",
    "ax.hlines(range(7), xmin=0, xmax=3, lw=1)\n",
    "ax.vlines(range(4, 6), ymin=0, ymax=6, lw=1)\n",
    "ax.hlines(range(7), xmin=4, xmax=5, lw=1)\n",
    "ax.text(1.5, 6.5, \"input / data / X\", size=14, ha='center')\n",
    "ax.text(4.6, 6.5, \"target / y\", size=14, ha='center')\n",
    "ax.text(0, -0.3, r'features $\\longrightarrow$')\n",
    "samples = r'$\\longleftarrow$ samples'\n",
    "ax.text(0, 6, samples, rotation=90, va='top', ha='right')\n",
    "ax.text(5.1, 6, samples, rotation=90, va='top', ha='left')\n",
    "ax.set_ylim(-1, 7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first one may argue that this will only work for two dimensions,\n",
    "but if you remember the ideas of *group by* and *pivot* we can see that\n",
    "we can represent several dimensions in a two dimensional table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "What the hell are those *hyperparameters*?\n",
    "These are simply free parameters that interfere in the inner workings of the model.\n",
    "By *free* we mean that the model works independently of what we set these parameters to.\n",
    "Yet, the values we give influence how well the model performs against a specific dataset.\n",
    "\n",
    "Therefore yes, the entire practice of machine learning is about finding the right model\n",
    "and the right model hyperparameters.  This could not be harder though, the combination of\n",
    "several free parameters goes into millions or thousands of millions for some models.\n",
    "\n",
    "As a simple example let's implement $k$ nearest neighbors, a model that has a single\n",
    "hyperparameter ($k$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyKNN(object):\n",
    "    def __init__(self, k=1):\n",
    "        self.k_ = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, new_X):\n",
    "        sq_dist_dim = (new_X[:, np.newaxis, :] - self.X[np.newaxis, :, :])**2\n",
    "        sq_dist = sq_dist_dim.sum(axis=-1)\n",
    "        self.nearest_ = np.argpartition(sq_dist, self.k_, axis=1)\n",
    "        new_y = self.y[self.nearest_[:, :self.k_]]\n",
    "        new_y = np.apply_along_axis(lambda x: np.bincount(x, minlength=2), axis=1, arr=new_y)\n",
    "        return new_y.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual work in our algorithm happen during `argpartition`,\n",
    "which sorts the points according to their distances from each point stored in the class.\n",
    "The we just sum together the classes of all close points (`bincount`),\n",
    "and output the most common class (`argmax`).\n",
    "\n",
    "Our implementation is imperfect,\n",
    "in case of a tie the algorithm prefers the class with the lowest index.\n",
    "But this is irrelevant if we use our algorithm as a binary classifier and with an odd $k$.\n",
    "A full implementation will need to deal with the ties.\n",
    "\n",
    "Nevertheless, we can try out our algorithm by classifying some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 27\n",
    "rng = np.random.RandomState(random_state)\n",
    "X, y = make_blobs(30, 2, centers=2, cluster_std=0.2, center_box=(0, 1), random_state=random_state)\n",
    "new_X = rng.rand(6, 2)\n",
    "knn_left = MyKNN(k=1)\n",
    "knn_left.fit(X, y)\n",
    "new_yl = knn_left.predict(new_X)\n",
    "knn_right = MyKNN(k=5)\n",
    "knn_right.fit(X, y)\n",
    "new_yr = knn_right.predict(new_X)\n",
    "fig, ax = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.subplots_adjust(hspace=0, wspace=0)\n",
    "ax[0, 0].set_title('k =  1')\n",
    "ax[0, 0].scatter(X[:, 0], X[:, 1], s=50, c=y, cmap='viridis')\n",
    "ax[0, 0].scatter(new_X[:, 0], new_X[:, 1], s=200)\n",
    "ax[1, 0].scatter(X[:, 0], X[:, 1], s=50, c=y, cmap='viridis')\n",
    "ax[1, 0].scatter(new_X[:, 0], new_X[:, 1], s=200, c=new_yl, cmap='viridis')\n",
    "ax[0, 1].set_title('k =  5')\n",
    "ax[0, 1].scatter(X[:, 0], X[:, 1], s=50, c=y, cmap='viridis')\n",
    "ax[0, 1].scatter(new_X[:, 0], new_X[:, 1], s=200)\n",
    "ax[1, 1].scatter(X[:, 0], X[:, 1], s=50, c=y, cmap='viridis')\n",
    "ax[1, 1].scatter(new_X[:, 0], new_X[:, 1], s=200, c=new_yr, cmap='viridis')\n",
    "for axi in ax.flat:\n",
    "    axi.axis('equal')\n",
    "    axi.set_xticks([], [])\n",
    "    axi.set_yticks([], [])\n",
    "new_yl, new_yr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm works but the difference in the classification when we change $k$.\n",
    "We need a way to tell what is a good value for $k$.\n",
    "Since we have several points for which we know the label already we can use\n",
    "them to check whether we get the right label.  This should be easy.\n",
    "\n",
    "How **not** to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = MyKNN(k=1)\n",
    "knn.fit(X, y)\n",
    "new_y = knn.predict(X)\n",
    "error = (y - new_y)**2\n",
    "error.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perfect model!  Which means $k = 5$ must be wrong, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = MyKNN(k=5)\n",
    "knn.fit(X, y)\n",
    "new_y = knn.predict(X)\n",
    "error = (y - new_y)**2\n",
    "error.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is something fishy about this.  Can you tell?\n",
    "\n",
    "Let's get some help from `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=random_state, test_size=0.3)\n",
    "accuracy = []\n",
    "f1_scores = []\n",
    "k_vals = list(range(1, 16))\n",
    "for k in k_vals:\n",
    "    knn = MyKNN(k=k)\n",
    "    knn.fit(train_X, train_y)\n",
    "    new_y = knn.predict(test_X)\n",
    "    accuracy.append(accuracy_score(test_y, new_y))\n",
    "    f1_scores.append(f1_score(test_y, new_y))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot(k_vals, accuracy, color='crimson')\n",
    "ax.plot(k_vals, f1_scores, color='steelblue');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks promising let's see how this goes on a bigger set.\n",
    "And we should try it a couple of times\n",
    "to see if we see any patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_k(ax):\n",
    "    X, y = make_blobs(512, 2, centers=2, cluster_std=0.2, center_box=(0, 1))\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3)\n",
    "    accuracy = []\n",
    "    f1_scores = []\n",
    "    k_vals = list(range(1, 256))\n",
    "    for k in k_vals:\n",
    "        knn = MyKNN(k=k)\n",
    "        knn.fit(train_X, train_y)\n",
    "        new_y = knn.predict(test_X)\n",
    "        accuracy.append(accuracy_score(test_y, new_y))\n",
    "        f1_scores.append(f1_score(test_y, new_y))\n",
    "    ax.plot(k_vals, accuracy, color='crimson')\n",
    "    ax.plot(k_vals, f1_scores, color='steelblue')\n",
    "    ax.set(xlabel='$k$', ylabel='score')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(16, 14), sharex=True)\n",
    "fig.subplots_adjust(hspace=0)\n",
    "for axi in ax.flat:\n",
    "    select_k(axi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are [many, many ways to calculate the \"correctness\" of what a model did spit out][metrics].\n",
    "And several of these are better for specific problems than others.  As a rule of thumb,\n",
    "a \"good enough\" first try measure is the F1 score, which performs well enough for most problems.\n",
    "It is defined in terms of *precision* and *recall*:\n",
    "\n",
    "#### precision\n",
    "\n",
    "$$P = \\frac{true\\:positives}{true\\:positives + false\\:positives}$$\n",
    "\n",
    "#### recall\n",
    "\n",
    "$$R = \\frac{true\\:positives}{true\\:positives + false\\:negatives}$$\n",
    "\n",
    "#### F1 score\n",
    "\n",
    "$$F_1 = \\frac{1}{\\frac{1}{P} + \\frac{1}{R}}=2 \\frac{P \\cdot R}{P + R}$$\n",
    "\n",
    "In this model we have only a single free parameter,\n",
    "and it is integer valued which makes it easy to select by hand.\n",
    "Some models have dozens of real valued parameters making the search\n",
    "for an optimal parameter much harder.\n",
    "We will see that when we look at these models.\n",
    "\n",
    "[metrics]: http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "-   [Evaluation of retrieved sets/classes - IR intro][1]\n",
    "\n",
    "[1]: https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-unranked-retrieval-sets-1.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
